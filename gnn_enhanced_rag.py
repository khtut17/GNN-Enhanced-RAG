# -*- coding: utf-8 -*-
"""GNN_Enhanced_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wbjq8aih8LljqlwLkfrWeex9w3XWnFqF
"""

# ================================
# GNN-RAG SYSTEM
# Graph Neural Network Enhanced RAG
# ================================

import os
os.environ['PYTHONHASHSEED'] = '0'
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

print("Initialising GNN-RAG System...")
print("=" * 60)

# Installing required packages
import subprocess
import sys

def install_package(package):
    """Install package only if not already installed"""
    try:
        __import__(package.replace('-', '_').split('[')[0])
        print(f"✓ {package} already installed")
        return True
    except ImportError:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package, "-q"])
            print(f"✓ {package} installed successfully")
            return True
        except Exception as e:
            print(f"✗ Failed to install {package}: {e}")
            return False

packages = [
    "torch-geometric",
    "networkx",
    "matplotlib",
    "faiss-cpu",  # For fast similarity search
    "transformers",  # For advanced NLP components
    "scikit-learn",
    "pandas",
    "numpy",
    "tqdm",
    "seaborn"]

print("Checking and installing required packages...")
failed_packages = []
for pkg in packages:
    if not install_package(pkg):
        failed_packages.append(pkg)

if failed_packages:
    print(f"\n⚠ Warning: Failed to install: {', '.join(failed_packages)}")
    print("You may need to install these manually")

print("\n Importing core libraries...")

# Core imports
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch_geometric.nn import RGCNConv, GATConv
    from torch_geometric.datasets import FB15k_237
    from torch_geometric.data import Data
    from torch_geometric.utils import softmax
    from datetime import datetime
    from scipy import stats
    from scipy.stats import wilcoxon, mannwhitneyu
    from tqdm import tqdm
    import urllib.request
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    import networkx as nx
    import random
    import time
    import re
    import json
    import faiss
    import gc
    import math
    import heapq, hashlib
    import getpass
    from typing import List, Tuple, Dict, Set, Optional
    from collections import defaultdict, deque, Counter
    from sklearn.metrics import accuracy_score, f1_score
    from statsmodels.stats.contingency_tables import mcnemar
    import warnings
    warnings.filterwarnings('ignore')

    print(" All imports successful")
except ImportError as e:
    print(f"✗ Import error: {e}")
    print("Please install missing packages manually")

# Setting seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n Device: {device}")
if device.type == 'cuda':
    print(f" GPU: {torch.cuda.get_device_name()}")
    print(f" GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

print("\n Environment setup complete!")
print("=" * 60)

# ================================
# STEP 2: DATASET LOADING
# ================================
print("\n STEP 2: Dataset Loading & Analysis")
print("=" * 60)

# --- Small utility: downloading a file if missing ---
def safe_download(url: str, dest_path: str):
    try:
        if os.path.exists(dest_path):
            return
        dir_path = os.path.dirname(dest_path)
        if dir_path:  # Only create directory if path has one
            os.makedirs(dir_path, exist_ok=True)
        print(f"  • Downloading {url}")
        urllib.request.urlretrieve(url, dest_path)
        print(f"  • Saved to {dest_path}")
    except Exception as e:
        print(f"  • WARNING: failed to download {url}: {e}")

# --- FB15k-237 GitHub raw locations ---
RAW_BASE = "https://raw.githubusercontent.com/villmow/datasets_knowledge_embedding/master/FB15k-237"
ENTITY2WIKI_URL = f"{RAW_BASE}/entity2wikidata.json"
TRAIN_URL = f"{RAW_BASE}/train.txt"
VALID_URL = f"{RAW_BASE}/valid.txt"
TEST_URL = f"{RAW_BASE}/test.txt"

class AdvancedDatasetLoader:
    """Dataset loader with graph analysis + real entity mapping via Wikidata IDs"""

    def __init__(self, dataset_name="FB15k_237", root_path="/content/fb15k_data"):
        self.dataset_name = dataset_name
        self.root_path = root_path
        self.dataset = None
        self.data = None
        self.statistics = {}

        # Storing graph patterns for better retrieval
        self.relation_patterns = defaultdict(list)
        self.entity_types = {}
        self.multi_hop_patterns = []

        # Mappings we'll populate:
        self.id2entity = None  # int id -> original FB entity string
        self.entity2id = None  # inverse
        self.entity2wikidata = None  # FB entity string -> Wikidata label
        self.id2wikidata = None  # int id -> Wikidata label

        # Relation mappings
        self.id2relation = {}  # int id -> relation name
        self.relation2id = {}  # relation name -> int id

    def _raw_dirs(self):
        raw_dir = getattr(self.dataset, "raw_dir", os.path.join(self.root_path, self.dataset_name, "raw"))
        processed_dir = getattr(self.dataset, "processed_dir", os.path.join(self.root_path, self.dataset_name, "processed"))
        return raw_dir, processed_dir

    def _download_fb15k_files_locally(self, raw_dir):
        safe_download(TRAIN_URL, os.path.join(raw_dir, "train.txt"))
        safe_download(VALID_URL, os.path.join(raw_dir, "valid.txt"))
        safe_download(TEST_URL, os.path.join(raw_dir, "test.txt"))
        safe_download(ENTITY2WIKI_URL, os.path.join(raw_dir, "entity2wikidata.json"))

    def _load_entity2wikidata(self, raw_dir):
        """Entity loading with proper cleaning"""
        path = os.path.join(raw_dir, "entity2wikidata.json")
        try:
            with open(path, "r") as f:
                raw_data = json.load(f)

            self.entity2wikidata = {}
            for fb_id, wiki_value in raw_data.items():
                # Cleaning the Wikidata value
                wiki_clean = str(wiki_value).strip()

                # Removing URL prefix if present
                if "/" in wiki_clean:
                    wiki_clean = wiki_clean.rsplit("/", 1)[-1]

                # Removing quotes and special chars BUT keeping the structure
                # The '}' might be part of malformed data, so here we handle it carefully
                wiki_clean = wiki_clean.strip("'\"")
                if wiki_clean.endswith("'}") or wiki_clean.endswith("}"):
                    wiki_clean = wiki_clean.rstrip("}'").rstrip("}")

                # Replacing underscores with spaces for readability
                wiki_clean = wiki_clean.replace('_', ' ')

                # Storing the cleaned label
                self.entity2wikidata[fb_id] = wiki_clean

            print(f"  • Loaded entity2wikidata.json ({len(self.entity2wikidata):,} entries)")
        except Exception as e:
            print(f"  • WARNING: could not read entity2wikidata.json: {e}")
            self.entity2wikidata = {}

    def _rebuild_id_maps_from_raw_triples(self, raw_dir):
        """Rebuilds entity AND relation mappings from raw triples"""
        files = [os.path.join(raw_dir, "train.txt"),
                 os.path.join(raw_dir, "valid.txt"),
                 os.path.join(raw_dir, "test.txt")]

        entity_seen = {}
        entity_order = []
        relation_seen = {}
        relation_order = []

        for fp in files:
            if not os.path.exists(fp):
                continue
            with open(fp, "r") as f:
                for line in f:
                    parts = line.strip().split('\t')
                    if len(parts) != 3:
                        parts = line.strip().split()
                        if len(parts) != 3:
                            continue

                    h, r, t = parts

                    # Tracking entities
                    if h not in entity_seen:
                        entity_seen[h] = len(entity_order)
                        entity_order.append(h)
                    if t not in entity_seen:
                        entity_seen[t] = len(entity_order)
                        entity_order.append(t)

                    # Tracking relations
                    if r not in relation_seen:
                        relation_seen[r] = len(relation_order)
                        relation_order.append(r)

        # Setting entity mappings
        self.entity2id = entity_seen
        self.id2entity = {idx: ent for ent, idx in entity_seen.items()}

        # Setting relation mappings
        self.relation2id = relation_seen
        self.id2relation = {idx: rel for rel, idx in relation_seen.items()}

        print(f"  • Rebuilt entity maps: {len(self.id2entity):,} entities")
        print(f"  • Rebuilt relation maps: {len(self.id2relation):,} relations")

    def _build_id2wikidata(self):
        """Builds integer id -> Wikidata label mapping"""
        id2wiki = {}
        missing = 0

        for idx, ent in self.id2entity.items():
            wiki_label = self.entity2wikidata.get(ent)
            if wiki_label:
                id2wiki[idx] = wiki_label
            else:
                missing += 1
                id2wiki[idx] = None

        found = len(id2wiki) - missing
        print(f"  • Resolved {found:,} entities to Wikidata labels; {missing:,} missing")
        self.id2wikidata = id2wiki

    def _create_relation_names(self):
        """Creates human-readable relation names"""
        # Since we don't have semantic labels, we'll create descriptive generic names
        relation_names = {}
        for rid, fb_rel in self.id2relation.items():
            if fb_rel.startswith('/'):
                # It's a Freebase relation - extracting the last meaningful part
                parts = fb_rel.strip('/').split('/')
                if len(parts) >= 2:
                    # Using last two parts for clarity
                    relation_names[rid] = f"{parts[-2]}.{parts[-1]}"
                else:
                    relation_names[rid] = parts[-1] if parts else f"R{rid}"
            else:
                # Generic fallback
                relation_names[rid] = f"R{rid}"

        return relation_names

    def load_and_analyze(self):
        """Loads dataset with comprehensive analysis"""
        print(f"  Loading {self.dataset_name} dataset...")

        # Loading FB15k-237
        self.dataset = FB15k_237(root=self.root_path)
        self.data = self.dataset[0].to(device)

        # Downloading files and loading mappings
        raw_dir, processed_dir = self._raw_dirs()
        self._download_fb15k_files_locally(raw_dir)
        self._load_entity2wikidata(raw_dir)

        # Rebuilding mappings from raw triples
        self._rebuild_id_maps_from_raw_triples(raw_dir)

        # Building derived mappings
        self._build_id2wikidata()

        # Creating relation names
        self.relation_names = self._create_relation_names()

        # Verifying mappings
        print("\n   Mapping Verification:")
        print(f"  • Entity IDs in data: 0-{self.data.num_nodes-1}")
        print(f"  • Mapped entities: {len(self.id2entity)}")
        print(f"  • Relation IDs in data: 0-{int(self.data.edge_type.max())}")
        print(f"  • Mapped relations: {len(self.id2relation)}")

        # Sample check
        print("\n   Sample Entity Mappings:")
        sample_ids = [32, 90, 434, 160, 141]
        for eid in sample_ids[:3]:
            if eid in self.id2entity:
                fb = self.id2entity.get(eid, "Unknown")
                wiki = self.id2wikidata.get(eid, "Unknown")
                print(f"    ID {eid}: {wiki if wiki else f'Entity_{eid}'}")

        print("\n   Sample Relation Mappings:")
        for rid in range(min(5, len(self.id2relation))):
            rname = self.relation_names.get(rid, f"R{rid}")
            print(f"    ID {rid}: {rname}")

        # Basic statistics (with safety check)
        avg_degree = self.data.edge_index.shape[1] / self.data.num_nodes if self.data.num_nodes > 0 else 0

        self.statistics = {
            'num_entities': self.data.num_nodes,
            'num_relations': int(self.data.edge_type.max()) + 1,
            'num_triples': self.data.edge_index.shape[1],
            'avg_degree': avg_degree}

        print(f"\n   Dataset loaded successfully!")
        self._print_statistics()
        self._analyze_graph_structure()
        self._analyze_relation_patterns()
        self._identify_reasoning_paths()

        return self.data, self.statistics

    def _print_statistics(self):
        """Prints comprehensive dataset statistics"""
        print(f"\n Dataset Statistics:")
        print(f"  • Entities: {self.statistics['num_entities']:,}")
        print(f"  • Relations: {self.statistics['num_relations']:,}")
        print(f"  • Triples: {self.statistics['num_triples']:,}")
        print(f"  • Avg Degree: {self.statistics['avg_degree']:.2f}")

    def _analyze_graph_structure(self):
        """Graph structure analysis"""
        print(f"\n Graph Structure Analysis:")

        edge_index = self.data.edge_index.cpu()
        all_nodes = torch.cat([edge_index[0], edge_index[1]])
        degrees = torch.bincount(all_nodes, minlength=self.data.num_nodes)
        degrees_float = degrees.float()

        self.statistics.update({
            'max_degree': degrees.max().item(),
            'min_degree': degrees.min().item(),
            'median_degree': degrees.median().item(),
            'degree_std': degrees_float.std().item(),
            'high_degree_nodes': (degrees > 100).sum().item(),
            'isolated_nodes': (degrees == 0).sum().item()})

        mean_degree = degrees_float.mean()
        std_degree = degrees_float.std()
        self.statistics['hub_nodes'] = (degrees > (mean_degree + 2 * std_degree)).sum().item()
        self.statistics['bridge_nodes'] = self._identify_bridge_nodes()

        print(f"  • Max degree: {self.statistics['max_degree']:,}")
        print(f"  • Hub nodes (>2σ): {self.statistics['hub_nodes']:,}")
        print(f"  • Bridge nodes: {self.statistics['bridge_nodes']:,}")
        print(f"  • Isolated Nodes: {self.statistics['isolated_nodes']}")

    def _analyze_relation_patterns(self):
        """Analyses relation composition patterns for multi-hop reasoning"""
        print(f"\n Analysing Relation Patterns:")

        edge_index = self.data.edge_index.cpu().numpy()
        edge_types = self.data.edge_type.cpu().numpy()

        adjacency = defaultdict(list)
        for i in range(edge_index.shape[1]):
            h, t = edge_index[0, i], edge_index[1, i]
            r = edge_types[i]
            adjacency[h].append((t, r))

        two_hop_patterns = defaultdict(int)
        sample_size = min(1000, len(adjacency))
        sample_nodes = random.sample(list(adjacency.keys()), sample_size)

        for node in sample_nodes:
            for neighbor1, rel1 in adjacency[node]:
                if neighbor1 in adjacency:
                    for neighbor2, rel2 in adjacency[neighbor1]:
                        pattern = (rel1, rel2)
                        two_hop_patterns[pattern] += 1

        top_patterns = sorted(two_hop_patterns.items(), key=lambda x: x[1], reverse=True)[:10]
        self.relation_patterns['2-hop'] = top_patterns

        print(f"  • Found {len(two_hop_patterns)} unique 2-hop patterns")
        print(f"  • Top 3 patterns: {[f'R{p[0][0]}→R{p[0][1]}' for p in top_patterns[:3]]}")

    def _identify_bridge_nodes(self):
        """Identifies nodes that connect different graph communities more accurately"""
        edge_index = self.data.edge_index.cpu()

        num_samples = min(100, self.data.num_nodes // 10)
        sample_nodes = random.sample(range(self.data.num_nodes), num_samples)

        adjacency = defaultdict(set)
        for i in range(edge_index.shape[1]):
            h, t = edge_index[0, i].item(), edge_index[1, i].item()
            adjacency[h].add(t)

        path_nodes = defaultdict(int)

        for i, source in enumerate(sample_nodes[:20]):
            for target in sample_nodes[i+1:i+10]:
                if source != target:
                    visited = {source}
                    queue = [(source, [source])]
                    found = False
                    while queue and not found:
                        node, path = queue.pop(0)
                        if node == target:
                            for n in path[1:-1]:
                                path_nodes[n] += 1
                            found = True
                            break
                        if len(path) < 4:
                            for neighbor in adjacency.get(node, []):
                                if neighbor not in visited:
                                    visited.add(neighbor)
                                    queue.append((neighbor, path + [neighbor]))

        threshold = max(1, len(path_nodes) * 0.1) if path_nodes else 1
        bridge_nodes = [n for n, count in path_nodes.items() if count >= threshold]
        return len(bridge_nodes)

    def _identify_reasoning_paths(self):
        """Identifies common reasoning path templates"""
        print(f"\n Identifying Reasoning Path Templates:")

        edge_index = self.data.edge_index.cpu().numpy()
        edge_types = self.data.edge_type.cpu().numpy()
        G = nx.DiGraph()
        for i in range(edge_index.shape[1]):
            G.add_edge(edge_index[0, i], edge_index[1, i], relation=edge_types[i])

        hub_nodes = self.get_hub_entities(top_k=20)[0]
        path_templates = defaultdict(int)

        for i, source in enumerate(hub_nodes[:10]):
            for target in hub_nodes[i+1:i+5]:
                try:
                    paths = list(nx.all_simple_paths(G, source, target, cutoff=3))
                    for path in paths[:5]:
                        if len(path) > 2:
                            relations = []
                            for j in range(len(path)-1):
                                if G.has_edge(path[j], path[j+1]):
                                    relations.append(G[path[j]][path[j+1]]['relation'])
                            if relations:
                                template = tuple(relations)
                                path_templates[template] += 1
                except (nx.NetworkXNoPath, nx.NodeNotFound):
                    continue

        self.multi_hop_patterns = sorted(path_templates.items(), key=lambda x: x[1], reverse=True)[:10]
        print(f"  • Found {len(path_templates)} unique path templates")
        if self.multi_hop_patterns:
            print(f"  • Most common: {len(self.multi_hop_patterns[0][0])}-hop paths")

        self.statistics['num_path_templates'] = len(path_templates)
        self.statistics['common_path_templates'] = self.multi_hop_patterns[:5]

    def get_hub_entities(self, top_k=10):
        edge_index = self.data.edge_index.cpu()
        all_nodes = torch.cat([edge_index[0], edge_index[1]])
        degrees = torch.bincount(all_nodes, minlength=self.data.num_nodes)
        top_entities = degrees.argsort(descending=True)[:top_k]
        return top_entities.tolist(), degrees[top_entities].tolist()

    def get_entity_neighborhood(self, entity_id, max_hops=2):
        edge_index = self.data.edge_index.cpu().numpy()
        edge_types = self.data.edge_type.cpu().numpy()
        adjacency = defaultdict(list)
        for i in range(edge_index.shape[1]):
            adjacency[edge_index[0, i]].append((edge_index[1, i], edge_types[i]))
        visited = set()
        queue = [(entity_id, 0)]
        neighborhood = defaultdict(list)
        while queue:
            node, hop = queue.pop(0)
            if hop > max_hops or node in visited:
                continue
            visited.add(node)
            if node in adjacency:
                for neighbor, relation in adjacency[node]:
                    neighborhood[hop].append((node, relation, neighbor))
                    if hop < max_hops:
                        queue.append((neighbor, hop + 1))
        return neighborhood

    def create_reasoning_benchmark(self, num_questions=200):
        print(f"\n Creating Reasoning Benchmark Dataset:")
        benchmark = []
        num_edges = self.data.edge_index.shape[1]

        # Type 1: Direct relation questions (25%)
        num_direct = num_questions // 4
        used_edges = set()
        for _ in range(num_direct * 2):
            if len(benchmark) >= num_direct:
                break
            idx = random.randint(0, num_edges - 1)
            if idx not in used_edges:
                used_edges.add(idx)
                h = self.data.edge_index[0, idx].item()
                t = self.data.edge_index[1, idx].item()
                r = self.data.edge_type[idx].item()
                benchmark.append({
                    'type': '1-hop',
                    'source': h,
                    'target': t,
                    'relation_path': [r],
                    'question_template': f"What is the relation between E{h} and E{t}?"
                })

        # Type 2: Multi-hop with guaranteed 2-hop/3-hop share
        num_multihop = int(num_questions * 0.35)

        # Gathering candidate 2-hop patterns from relation_patterns
        two_hop_candidates = []
        if '2-hop' in self.relation_patterns and self.relation_patterns['2-hop']:
            two_hop_candidates = [(p, c) for (p, c) in self.relation_patterns['2-hop'] if len(p) == 2]

        # Gathering candidate 3-hop patterns from multi_hop_patterns
        three_hop_candidates = [(p, c) for (p, c) in self.multi_hop_patterns if len(p) == 3]

        # Sorting by frequency
        two_hop_candidates.sort(key=lambda x: x[1], reverse=True)
        three_hop_candidates.sort(key=lambda x: x[1], reverse=True)

        if num_multihop > 0:
            min_2hop = max(1, int(num_multihop * 0.4))
            min_3hop = max(1, int(num_multihop * 0.4))
            remaining = num_multihop - (min_2hop + min_3hop)

            def take_with_reuse(cands, k):
                if not cands:
                    return []
                out, i = [], 0
                while len(out) < k:
                    out.append(cands[i % len(cands)])
                    i += 1
                return out

            chosen_2hop = take_with_reuse(two_hop_candidates, min_2hop)
            for pattern, count in chosen_2hop:
                benchmark.append({
                    'type': '2-hop',
                    'pattern': tuple(pattern),
                    'count': int(count),
                    'question_template': f"Find entities connected by 2-hop path with relations {tuple(pattern)}"})

            if three_hop_candidates:
                chosen_3hop = take_with_reuse(three_hop_candidates, min_3hop)
                for pattern, count in chosen_3hop:
                    benchmark.append({
                        'type': '3-hop',
                        'pattern': tuple(pattern),
                        'count': int(count),
                        'question_template': f"Find entities connected by 3-hop path with relations {tuple(pattern)}"})
            else:
                extra_2hop = take_with_reuse(two_hop_candidates, min_3hop)
                for pattern, count in extra_2hop:
                    benchmark.append({
                        'type': '2-hop',
                        'pattern': tuple(pattern),
                        'count': int(count),
                        'question_template': f"Find entities connected by 2-hop path with relations {tuple(pattern)}"})

            pool_rest = three_hop_candidates[min_3hop:] + two_hop_candidates[min_2hop:]
            if pool_rest:
                filler = take_with_reuse(pool_rest, max(0, remaining))
                for pattern, count in filler:
                    benchmark.append({
                        'type': f'{len(pattern)}-hop',
                        'pattern': tuple(pattern),
                        'count': int(count),
                        'question_template': f"Find entities connected by {len(pattern)}-hop path with relations {tuple(pattern)}"})

        # Type 3: Similarity (25%)
        num_similarity = num_questions // 4
        all_entities = list(range(self.data.num_nodes))
        sampled_entities = random.sample(all_entities, min(num_similarity, len(all_entities)))
        for entity in sampled_entities:
            benchmark.append({
                'type': 'similarity',
                'source': entity,
                'question_template': f"What entities are most similar to E{entity}?"})

        # Type 4: Neighbourhood (25%)
        num_neighborhood = num_questions // 4
        hub_entities, _ = self.get_hub_entities(top_k=num_neighborhood * 2)
        for entity in hub_entities[:num_neighborhood]:
            benchmark.append({
                'type': 'neighborhood',
                'source': entity,
                'question_template': f"What are the main connections of E{entity}?"})

        # Type 5: Path existence (fill remainder)
        remaining = num_questions - len(benchmark)
        if remaining > 0:
            for _ in range(remaining):
                source = random.randint(0, self.data.num_nodes - 1)
                target = random.randint(0, self.data.num_nodes - 1)
                if source != target:
                    benchmark.append({
                        'type': 'path-existence',
                        'source': source,
                        'target': target,
                        'question_template': f"Is there a path between E{source} and E{target}?"})

        random.shuffle(benchmark)
        self.benchmark_questions = benchmark
        print(f"  • Created {len(benchmark)} benchmark questions")

        type_counts = {}
        for q in benchmark:
            type_counts[q['type']] = type_counts.get(q['type'], 0) + 1
        print(f"  • Question distribution:")
        for q_type, count in sorted(type_counts.items()):
            print(f"    - {q_type}: {count} ({count/len(benchmark)*100:.1f}%)")

        multi_counts = {'2-hop': 0, '3-hop': 0}
        for q in benchmark:
            if q['type'] == '2-hop':
                multi_counts['2-hop'] += 1
            elif q['type'] == '3-hop':
                multi_counts['3-hop'] += 1
        print(f"  • Multi-hop breakdown:")
        for hop_type, count in multi_counts.items():
            print(f"    - {hop_type}: {count} ({count/len(benchmark)*100:.1f}%)")

        return benchmark

    def save_benchmark(self, filepath='benchmark_questions.json'):
        """Saves benchmark questions for consistent evaluation"""
        if hasattr(self, 'benchmark_questions'):
            def convert_to_native(obj):
                if isinstance(obj, np.integer):  return int(obj)
                if isinstance(obj, np.floating): return float(obj)
                if isinstance(obj, np.ndarray):  return obj.tolist()
                if isinstance(obj, dict):        return {k: convert_to_native(v) for k, v in obj.items()}
                if isinstance(obj, list):        return [convert_to_native(i) for i in obj]
                if isinstance(obj, tuple):       return tuple(convert_to_native(i) for i in obj)
                return obj
            with open(filepath, 'w') as f:
                json.dump(convert_to_native(self.benchmark_questions), f, indent=2)
            print(f"  • Benchmark saved to {filepath}")
        else:
            print("  • No benchmark to save - creating one first")

    def load_benchmark(self, filepath='benchmark_questions.json'):
        """Loads previously saved benchmark questions"""
        try:
            with open(filepath, 'r') as f:
                self.benchmark_questions = json.load(f)
            print(f"  • Loaded {len(self.benchmark_questions)} benchmark questions")
            return self.benchmark_questions
        except FileNotFoundError:
            print(f"  • No saved benchmark found at {filepath}")
            return None

# -------- Run loader + analysis --------
print("\n Initialising Dataset Loader...")
loader = AdvancedDatasetLoader()
data, stats = loader.load_and_analyze()

# Creating global entity names mapping
ENTITY_NAMES = {}
for idx in range(stats['num_entities']):
    wiki_name = loader.id2wikidata.get(idx)
    if wiki_name and wiki_name != "None":
        ENTITY_NAMES[idx] = wiki_name
    else:
        # Using Freebase ID if no Wikidata label
        fb_id = loader.id2entity.get(idx, None)
        if fb_id:
            ENTITY_NAMES[idx] = f"FB:{fb_id}"
        else:
            ENTITY_NAMES[idx] = f"Entity_{idx}"

# Creating global relation names mapping
RELATION_NAMES = loader.relation_names.copy()

# Creating benchmark dataset
benchmark = loader.create_reasoning_benchmark(num_questions=200)
loader.save_benchmark()  # Saving for consistent evaluation
loaded = loader.load_benchmark()  # Test loading
print(f" Successfully saved and loaded {len(loaded)} questions")

print(f"\n STEP 2 COMPLETE")
print(f"  • Entities mapped: {len(ENTITY_NAMES):,}")
print(f"  • Relations mapped: {len(RELATION_NAMES):,}")
print("=" * 60)

# Exporting key variables for next steps
NUM_ENTITIES = stats['num_entities']
NUM_RELATIONS = stats['num_relations']
NUM_TRIPLES = stats['num_triples']
RELATION_PATTERNS = loader.relation_patterns
MULTI_HOP_PATTERNS = loader.multi_hop_patterns
BENCHMARK_QUESTIONS = loader.benchmark_questions if hasattr(loader, 'benchmark_questions') else []

# These are now available globally:
# ENTITY_NAMES - dict mapping entity ID to readable name
# RELATION_NAMES - dict mapping relation ID to readable name

# ================================
# STEP 2.5: CREATING TRAIN/TEST SPLIT
# ================================
print("\n STEP 2.5: Creating Train/Test Split")
print("=" * 60)

def create_edge_split(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):
    """
    Standard knowledge graph split: hold out edges for testing.
    This prevents train/test contamination.
    """
    num_edges = data.edge_index.shape[1]
    perm = torch.randperm(num_edges)

    train_size = int(num_edges * train_ratio)
    val_size = int(num_edges * val_ratio)

    train_idx = perm[:train_size]
    val_idx = perm[train_size:train_size + val_size]
    test_idx = perm[train_size + val_size:]

    print(f"  • Total edges: {num_edges:,}")
    print(f"  • Train edges: {len(train_idx):,} ({train_ratio*100:.0f}%)")
    print(f"  • Val edges: {len(val_idx):,} ({val_ratio*100:.0f}%)")
    print(f"  • Test edges: {len(test_idx):,} ({test_ratio*100:.0f}%)")

    return {
        'train_edge_index': data.edge_index[:, train_idx],
        'train_edge_type': data.edge_type[train_idx],
        'val_edge_index': data.edge_index[:, val_idx],
        'val_edge_type': data.edge_type[val_idx],
        'test_edge_index': data.edge_index[:, test_idx],
        'test_edge_type': data.edge_type[test_idx],
        'train_idx': train_idx,
        'val_idx': val_idx,
        'test_idx': test_idx}

# Creating the split BEFORE any training
SPLIT_DATA = create_edge_split(data, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)

print("\n STEP 2.5 COMPLETE - Train/Test split created!")
print("=" * 60)

# ================================
# STEP 3: GNN ARCHITECTURE WITH NEURAL PATH GUIDANCE
# ================================

print("\n STEP 3: GNN Architecture with Neural Path Guidance")
print("=" * 60)

class RelationEdgeAttention(nn.Module):
    """
    Neighbour-wise relation-aware edge attention.
    Computes a scalar per edge with an MLP, squashes via sigmoid in (0,1),
    Then normalises per destination using pure PyTorch index_add_.
    """
    def __init__(self, hidden_dim, dropout=0.1, negative_slope=0.2):
        super().__init__()
        self.src_lin = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.dst_lin = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.rel_lin = nn.Linear(hidden_dim, hidden_dim, bias=False)
        self.attn_mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2, bias=True),
            nn.LeakyReLU(negative_slope),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1, bias=False))

    def forward(self, h, edge_index, edge_type, rel_embs):
        src, dst = edge_index  # [E], [E]
        h_src = h[src]         # [E, D]
        h_dst = h[dst]         # [E, D]
        r     = rel_embs[edge_type]  # [E, D]

        x = self.src_lin(h_src) + self.dst_lin(h_dst) + self.rel_lin(r)  # [E, D]
        x = F.gelu(x)
        logits = self.attn_mlp(x).squeeze(-1)     # [E]
        alpha_raw = torch.sigmoid(logits)         # (0,1)

        # Normalising alphas per destination node (sum-to-1 over incoming edges)
        num_nodes = h.size(0)
        denom = torch.zeros(num_nodes, device=h.device, dtype=alpha_raw.dtype)
        denom.index_add_(0, dst, alpha_raw)       # sum of alphas per dst node
        alpha = alpha_raw / (denom[dst] + 1e-9)   # [E]

        return alpha  # per-edge weights in [0,1], sum_j alpha_ij = 1 per dst i

class DynamicHopAggregator(nn.Module):
    """Learns to aggregate information from different hop distances (layers)."""
    def __init__(self, hidden_dim, num_layers):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.layer_weights = nn.Parameter(torch.ones(num_layers) / num_layers)
        self.question_gate = nn.Linear(hidden_dim * 2, num_layers, bias=True)

    def forward(self, layer_outputs, question_emb=None):
        stacked = torch.stack(layer_outputs, dim=1)  # [N, L, D]
        if question_emb is not None:
            question_emb = question_emb.to(stacked.device)
            # Validating question embedding shape
            if question_emb.dim() == 1:
                question_emb = question_emb.unsqueeze(0)  # [1, D]
            if question_emb.size(-1) != self.hidden_dim:
                raise ValueError(f"Question embedding dim {question_emb.size(-1)} != hidden_dim {self.hidden_dim}")

            graph_state = stacked.mean(dim=0).mean(dim=0)  # [D]
            logits = self.question_gate(torch.cat([graph_state, question_emb.squeeze(0)], dim=-1))  # [L]
            weights = F.softmax(logits, dim=-1)
        else:
            weights = F.softmax(self.layer_weights, dim=0)
        weights = weights.view(1, -1, 1)  # [1, L, 1]
        return (stacked * weights).sum(dim=1)  # [N, D]

class NeuralPathScorer(nn.Module):
    """Scores paths based on semantic coherence and question relevance."""
    def __init__(self, hidden_dim):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.coherence_scorer = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1))
        self.relevance_scorer = nn.Bilinear(hidden_dim, hidden_dim, 1)

    def forward(self, path_embs, question_emb):
        if isinstance(path_embs, list):
            if len(path_embs) == 0:
                device = question_emb.device if question_emb is not None else torch.device('cpu')
                zero = torch.tensor(0.0, device=device)
                return zero, {'coherence': zero, 'relevance': zero}
            path_embs = torch.stack(path_embs, dim=0)

        if path_embs.numel() == 0:
            device = question_emb.device if question_emb is not None else torch.device('cpu')
            zero = torch.tensor(0.0, device=device)
            return zero, {'coherence': zero, 'relevance': zero}

        # Pairwise coherence
        coh = []
        for i in range(len(path_embs) - 1):
            pair = torch.cat([path_embs[i], path_embs[i + 1]], dim=-1)
            coh.append(self.coherence_scorer(pair))
        coherence = torch.stack(coh).mean() if coh else torch.tensor(0.0, device=path_embs.device)

        # Question relevance
        path_repr = path_embs.mean(dim=0)
        relevance = self.relevance_scorer(path_repr, question_emb) if question_emb is not None \
                    else torch.tensor(0.0, device=path_embs.device)
        return coherence + relevance, {'coherence': coherence, 'relevance': relevance}

class NeuralPathGuidedRGCN(nn.Module):
    """
    R-GCN with Neural Path Guidance.

      • R-GCN stack + residual + LayerNorm + GELU
      • Neighbour-wise relation-aware edge attention (normalised per dst)
      • Dynamic hop aggregation
      • Optional question conditioning on the projector
    """
    def __init__(self, num_entities, num_relations, hidden_dim=256,
                 num_layers=4, num_heads=8, dropout=0.1, num_bases=32,
                 path_patterns=None):
        super().__init__()
        self.num_entities  = num_entities
        self.num_relations = num_relations
        self.hidden_dim    = hidden_dim
        self.num_layers    = num_layers
        self.num_heads     = num_heads  # For compatibility
        self.path_patterns = path_patterns

        print(f" Building Neural Path-Guided R-GCN:")
        print(f"  • Entities: {num_entities:,}")
        print(f"  • Relations: {num_relations}")
        print(f"  • Hidden Dim: {hidden_dim}")
        print(f"  • Layers: {num_layers}")
        print(f"  • Path Patterns: {len(path_patterns['2-hop']) if path_patterns else 0}")

        # Embeddings
        self.entity_embeddings   = nn.Embedding(num_entities, hidden_dim)
        self.relation_embeddings = nn.Embedding(num_relations, hidden_dim)

        # Path importance MLP
        self.path_scorer = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid())

        # Stacks
        self.rgcn_layers = nn.ModuleList()
        self.edge_attn   = nn.ModuleList()
        self.layer_norms = nn.ModuleList()

        # Residual attention branch projections
        self.msg_src_lin = nn.ModuleList()
        self.msg_rel_lin = nn.ModuleList()
        self.msg_proj    = nn.ModuleList()

        for _ in range(num_layers):
            self.rgcn_layers.append(
                RGCNConv(hidden_dim, hidden_dim, num_relations, num_bases=num_bases, aggr='mean'))
            self.edge_attn.append(RelationEdgeAttention(hidden_dim, dropout=dropout))
            self.layer_norms.append(nn.LayerNorm(hidden_dim))

            self.msg_src_lin.append(nn.Linear(hidden_dim, hidden_dim, bias=False))
            self.msg_rel_lin.append(nn.Linear(hidden_dim, hidden_dim, bias=False))
            self.msg_proj.append(nn.Linear(hidden_dim, hidden_dim, bias=False))

        # Hop aggregator
        self.hop_aggregator = DynamicHopAggregator(hidden_dim, num_layers)

        # Output projector (question-conditioned)
        self.entity_projector = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim))

        self._init_parameters()
        self._print_parameters()

    def _init_parameters(self):
        for name, p in self.named_parameters():
            if p.dim() > 1 and 'weight' in name:
                nn.init.xavier_uniform_(p)
            elif 'bias' in name:
                nn.init.constant_(p, 0.0)

    def _print_parameters(self):
        total = sum(p.numel() for p in self.parameters())
        train = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"\n Model Parameters:")
        print(f"  • Total: {total:,}")
        print(f"  • Trainable: {train:,}")
        print(f"  • Size: ~{total * 4 / 1024 / 1024:.1f} MB")

    def forward(self, edge_index, edge_type, question_emb=None, return_all=False):
        """
        Args:
            edge_index: [2, E]
            edge_type:  [E]
            question_emb: Optional [D] or [1, D]
        """
        # Initial embeddings
        h = F.normalize(self.entity_embeddings.weight, p=2, dim=1)  # [N, D]
        rel_embs = self.relation_embeddings.weight                   # [R, D]

        layer_outputs = []
        attn_layers   = []

        src, dst = edge_index  # [E], [E]
        num_nodes = h.size(0)

        for rgcn, attn, norm, src_lin, rel_lin, proj_lin in zip(
            self.rgcn_layers, self.edge_attn, self.layer_norms,
            self.msg_src_lin, self.msg_rel_lin, self.msg_proj):
            h_in = h

            # 1) Standard R-GCN
            h_rgcn = rgcn(h, edge_index, edge_type)  # [N, D]

            # 2) Relation-aware attention residual branch (edge-wise)
            alpha = attn(h, edge_index, edge_type, rel_embs)                # [E]
            edge_msg = src_lin(h[src]) + rel_lin(rel_embs[edge_type])       # [E, D]
            edge_msg = F.gelu(edge_msg)
            weighted = alpha.unsqueeze(-1) * edge_msg                        # [E, D]

            # Aggregating to destination using pure PyTorch
            attn_agg = torch.zeros(num_nodes, h.size(1), device=h.device, dtype=h.dtype)  # [N, D]
            attn_agg.index_add_(0, dst, weighted)                                         # sum over incoming edges
            attn_agg = proj_lin(attn_agg)                                                 # [N, D]

            # 3) Combine, norm, activation
            h = h_rgcn + attn_agg
            h = norm(h + h_in)
            h = F.gelu(h)

            layer_outputs.append(h)
            attn_layers.append(alpha.detach())

        # Dynamic hop aggregation
        h = self.hop_aggregator(layer_outputs, question_emb)  # [N, D]

        # Question conditioning
        if question_emb is not None:
            question_emb = question_emb.to(next(self.parameters()).device)
            if question_emb.dim() == 1:
                q = question_emb.unsqueeze(0).expand(h.size(0), -1)  # [N, D]
            else:
                q = question_emb.expand(h.size(0), -1)  # [N, D]
            h = torch.cat([h, q], dim=-1)  # [N, 2D]
        else:
            h = torch.cat([h, h], dim=-1)  # [N, 2D]

        h = F.normalize(h, p=2, dim=-1)
        entity_outputs = self.entity_projector(h)                # [N, D]
        entity_outputs = F.normalize(entity_outputs, p=2, dim=-1)

        if return_all:
            return {
                'entity_embeddings': entity_outputs,
                'relation_embeddings': rel_embs,
                'layer_outputs': layer_outputs,
                'attention_weights': attn_layers  # per-edge alphas per layer
            }

        return entity_outputs, rel_embs

    def compute_path_importance(self, path_entities, path_relations, question_emb=None):
        device = next(self.parameters()).device
        e_embs = self.entity_embeddings(torch.tensor(path_entities, device=device))     # [L+1, D]
        r_embs = self.relation_embeddings(torch.tensor(path_relations, device=device))  # [L, D]
        q_emb = question_emb if question_emb is not None else torch.zeros(self.hidden_dim, device=device)

        path_repr = torch.cat([
            e_embs.mean(dim=0),
            r_embs.mean(dim=0),
            q_emb.squeeze() if q_emb.dim() > 1 else q_emb
        ], dim=-1)
        score = self.path_scorer(path_repr)  # [1]
        return score.item()

# ---- Initialise model + path scorer ----
print("\n Initialising Neural Path-Guided R-GCN...")
model = NeuralPathGuidedRGCN(
    num_entities=NUM_ENTITIES,
    num_relations=NUM_RELATIONS,
    hidden_dim=256,
    num_layers=4,
    num_heads=8,     # For compatibility
    dropout=0.1,
    num_bases=64,
    path_patterns=RELATION_PATTERNS
).to(device)

# Using model's hidden_dim instead of hardcoding
path_scorer = NeuralPathScorer(hidden_dim=model.hidden_dim).to(device)

print(f"\n GNN Model Initialised!")

# ---- Quick forward checks (CPU-friendly) ----
print(f"\n Testing model forward pass...")
with torch.no_grad():
    # Without question
    entity_embs, relation_embs = model(data.edge_index, data.edge_type)
    print(f"  • Basic forward pass successful")
    print(f"    Entity embeddings: {tuple(entity_embs.shape)}")
    print(f"    Relation embeddings: {tuple(relation_embs.shape)}")

    # With question embedding
    dummy_question = torch.randn(256, device=device)
    results = model(data.edge_index, data.edge_type, dummy_question, return_all=True)
    print(f"  • Question-conditioned forward pass successful")
    print(f"    Layer outputs: {len(results['layer_outputs'])}")
    print(f"    Attention (per layer) captured: {len(results['attention_weights'])}")

    # Path importance sanity check with valid entities
    test_path = [32, 90, 434]  # USA -> USD -> Marriage (all exist from Step 2 output)
    test_rels = [6, 5]
    print(f"  • Test path entities: {[ENTITY_NAMES.get(e, f'Entity_{e}') for e in test_path]}")
    importance = model.compute_path_importance(test_path, test_rels, dummy_question)
    print(f"  • Path importance scoring successful")
    print(f"    Test path importance: {importance:.3f}")

print("\n" + "=" * 60)
print(" STEP 3 COMPLETE")
print("=" * 60)

# Exporting for later steps
GNN_MODEL = model
PATH_SCORER = path_scorer

# ================================
# STEP 4: TRAINING (TransE + GNN)
# ================================
print("\n STEP 4: Training TransE then GNN")
print("=" * 60)

# Training configuration
SEED = 42
BATCH_SIZE = 512
TRANS_EPOCHS = 25
GNN_EPOCHS = 15
PATIENCE = 5
LR_TRANSE = 0.01
LR_GNN = 0.0005
WEIGHT_DECAY = 2e-5
HIDDEN_DIM_TR = 128
PROJ_DIM = GNN_MODEL.hidden_dim
MARGIN = 1.5

random.seed(SEED)
torch.manual_seed(SEED)
np.random.seed(SEED)

# Cleaning up & setting device
gc.collect()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f" Using device: {device}")

# Ensuring data is on correct device
edge_index_dev = data.edge_index.long().to(device)
edge_type_dev = data.edge_type.long().to(device)
NUM_EDGES = edge_index_dev.size(1)

# ================================
# PART 1: TransE Training
# ================================
print("\n Part 1: Training TransE embeddings...")
print("-" * 40)

class EmbeddingTrainer:
    """Lightweight TransE trainer"""
    def __init__(self, num_entities, num_relations, hidden_dim=HIDDEN_DIM_TR):
        self.num_entities = num_entities
        self.num_relations = num_relations
        self.hidden_dim = hidden_dim

        self.entity_embeddings = nn.Embedding(num_entities, hidden_dim).to(device)
        self.relation_embeddings = nn.Embedding(num_relations, hidden_dim).to(device)

        nn.init.xavier_uniform_(self.entity_embeddings.weight)
        nn.init.xavier_uniform_(self.relation_embeddings.weight)

        self.optimizer = torch.optim.Adam(
            [self.entity_embeddings.weight, self.relation_embeddings.weight],
            lr=LR_TRANSE, weight_decay=WEIGHT_DECAY)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=2)

        print(f"  • Entities: {num_entities:,}")
        print(f"  • Relations: {num_relations}")
        print(f"  • Hidden dim: {hidden_dim}")
        print(f"  • Learning rate: {LR_TRANSE}")

    def _margin_rank_loss(self, pos_scores, neg_scores, margin=MARGIN):
        return F.relu(pos_scores - neg_scores + margin).mean()

    def train_batch(self, heads_idx, tails_idx, rel_idx):
        B = heads_idx.size(0)

        h = self.entity_embeddings(heads_idx)
        t = self.entity_embeddings(tails_idx)
        r = self.relation_embeddings(rel_idx)

        pos = torch.norm(h + r - t, p=2, dim=1)

        # Hard negative sampling
        corrupt_head = torch.rand(B, device=h.device) > 0.5

        with torch.no_grad():
            # For tail corruption: finding entities similar to h+r
            expected_tail = h + r
            sims = torch.mm(expected_tail, self.entity_embeddings.weight.t())
            sims[torch.arange(B), tails_idx] = -1e10  # mask correct answer

            # Sampling from top-50 similar entities
            k = min(50, self.num_entities // 10)
            _, top_indices = sims.topk(k, dim=1)
            selected_idx = torch.randint(0, top_indices.size(1), (B,), device=h.device)
            neg_idx = top_indices[torch.arange(B), selected_idx]

        h_neg = torch.where(corrupt_head.unsqueeze(1), self.entity_embeddings(neg_idx), h)
        t_neg = torch.where(~corrupt_head.unsqueeze(1), self.entity_embeddings(neg_idx), t)
        neg = torch.norm(h_neg + r - t_neg, p=2, dim=1)

        loss = self._margin_rank_loss(pos, neg, margin=MARGIN)

        norm_loss = (h.norm(2, dim=1).mean() + t.norm(2, dim=1).mean() + r.norm(2, dim=1).mean()) * 1e-3
        return loss + norm_loss

    def train(self, edge_index, edge_type, epochs=TRANS_EPOCHS):
        print("\n Starting TransE training...")

        E = edge_index.size(1)
        perm = torch.randperm(E, device=edge_index.device)
        train_size = int(0.9 * E)
        train_idx = perm[:train_size]
        val_idx = perm[train_size:]

        train_edges = edge_index[:, train_idx]
        train_types = edge_type[train_idx]
        val_edges = edge_index[:, val_idx]
        val_types = edge_type[val_idx]

        best_val = float('inf')
        patience_ctr = 0
        best_e, best_r = None, None
        last_transe_epoch = 0

        for epoch in range(1, epochs + 1):
            last_transe_epoch = epoch
            self.entity_embeddings.train()
            self.relation_embeddings.train()
            epoch_loss = 0.0
            num_batches = 0

            p = torch.randperm(train_edges.size(1), device=edge_index.device)
            for i in range(0, train_edges.size(1), BATCH_SIZE):
                self.optimizer.zero_grad(set_to_none=True)
                idx = p[i:i + BATCH_SIZE]

                heads_idx = train_edges[0, idx]
                tails_idx = train_edges[1, idx]
                rel_idx = train_types[idx]

                loss = self.train_batch(heads_idx, tails_idx, rel_idx)
                loss.backward()

                nn.utils.clip_grad_norm_(
                    [self.entity_embeddings.weight, self.relation_embeddings.weight],
                    max_norm=1.0)
                self.optimizer.step()
                epoch_loss += loss.item()
                num_batches += 1

            avg_loss = epoch_loss / num_batches

            self.entity_embeddings.eval()
            self.relation_embeddings.eval()
            with torch.no_grad():
                val_h = self.entity_embeddings(val_edges[0])
                val_t = self.entity_embeddings(val_edges[1])
                val_r = self.relation_embeddings(val_types)
                val_loss = torch.norm(val_h + val_r - val_t, p=2, dim=1).mean().item()

            old_lr = self.optimizer.param_groups[0]['lr']
            self.scheduler.step(val_loss)
            new_lr = self.optimizer.param_groups[0]['lr']

            # Manual verbose output
            if old_lr != new_lr:
                print(f"  • LR reduced: {old_lr:.5f} → {new_lr:.5f}")

            print(f"  Epoch {epoch:2d}/{epochs} | Train: {avg_loss:.4f} | Val: {val_loss:.4f} | LR: {new_lr:.5f}")

            if val_loss < best_val - 1e-6:
                best_val = val_loss
                patience_ctr = 0
                best_e = self.entity_embeddings.weight.detach().clone()
                best_r = self.relation_embeddings.weight.detach().clone()
            else:
                patience_ctr += 1
                if patience_ctr >= PATIENCE:
                    print(f"  • Early stopping at epoch {epoch}")
                    break

        if best_e is not None:
            with torch.no_grad():
                self.entity_embeddings.weight.copy_(best_e)
                self.relation_embeddings.weight.copy_(best_r)

        print(f" TransE training complete! Best val: {best_val:.4f}")
        return (
            self.entity_embeddings.weight.detach(),
            self.relation_embeddings.weight.detach(),
            last_transe_epoch)

# Training TransE
trainer = EmbeddingTrainer(NUM_ENTITIES, NUM_RELATIONS, hidden_dim=HIDDEN_DIM_TR)
transe_entity_embs, transe_relation_embs, last_transe_epoch = trainer.train(SPLIT_DATA['train_edge_index'].to(device),
              SPLIT_DATA['train_edge_type'].to(device))

# Projecting to GNN dimension
print(f"\n Projecting from {HIDDEN_DIM_TR}D to {PROJ_DIM}D...")
proj_e = torch.empty(HIDDEN_DIM_TR, PROJ_DIM, device=transe_entity_embs.device, dtype=transe_entity_embs.dtype)
proj_r = torch.empty(HIDDEN_DIM_TR, PROJ_DIM, device=transe_relation_embs.device, dtype=transe_relation_embs.dtype)
# Initialising projection matrices with small scale to preserve stability
nn.init.orthogonal_(proj_e)
proj_e.mul_(0.1)
nn.init.orthogonal_(proj_r)
proj_r.mul_(0.1)

transe_entity_embs_proj = F.normalize(transe_entity_embs @ proj_e, p=2, dim=1)
transe_relation_embs_proj = F.normalize(transe_relation_embs @ proj_r, p=2, dim=1)

# ================================
# PART 2: GNN Training
# ================================
print("\n Part 2: Training GNN with TransE initialisation...")
print("-" * 40)

# Initialising GNN with TransE embeddings
GNN_MODEL = GNN_MODEL.to(device)
with torch.no_grad():
    GNN_MODEL.entity_embeddings.weight.copy_(transe_entity_embs_proj)
    GNN_MODEL.relation_embeddings.weight.copy_(transe_relation_embs_proj)

print("  • GNN initialised with TransE embeddings")
print(f"  • GNN parameters: {sum(p.numel() for p in GNN_MODEL.parameters()):,}")

def train_gnn(model, edge_index, edge_type, epochs=GNN_EPOCHS):
    """
    GNN TRAINING - This performs actual gradient descent on ALL GNN parameters
    including R-GCN layers, attention mechanisms, and embeddings.
    """
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=LR_GNN, weight_decay=WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    num_edges = edge_index.shape[1]
    last_gnn_epoch = 0

    print("\n Starting GNN training (all parameters will be updated)...")
    print(f"  • Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    for epoch in range(1, epochs + 1):
        last_gnn_epoch = epoch
        model.train()  # Ensuring training mode
        epoch_loss = 0.0
        num_batches = 0
        perm = torch.randperm(num_edges, device=device)

        for i in range(0, num_edges, BATCH_SIZE * 2):
            # Zero gradients
            optimizer.zero_grad(set_to_none=True)

            # Getting batch
            batch_idx = perm[i:i + BATCH_SIZE * 2]
            batch_edges = edge_index[:, batch_idx]
            batch_types = edge_type[batch_idx]

            # CRITICAL: Forward pass through ENTIRE GNN (all 4 R-GCN layers + attention)
            # This is NOT just embedding lookup - it propagates through the full network
            entity_embs, rel_embs = model(edge_index, edge_type)

            # Computing TransE-style loss
            head = entity_embs[batch_edges[0]]
            tail = entity_embs[batch_edges[1]]
            rel  = rel_embs[batch_types]
            pos_score = torch.norm(head + rel - tail, p=2, dim=1)

            # Hard negative sampling
            corrupt_head = torch.rand(batch_edges.shape[1], device=device) > 0.5
            with torch.no_grad():
                expected = head + rel
                all_sims = torch.mm(expected, entity_embs.t())
                all_sims[torch.arange(batch_edges.shape[1]), batch_edges[1]] = -1e10
                k = min(50, model.num_entities // 10)
                _, top_k = all_sims.topk(k, dim=1)
                idx = torch.randint(0, top_k.size(1), (batch_edges.shape[1],), device=device)
                neg_ids = top_k[torch.arange(batch_edges.shape[1]), idx]

            neg_heads = torch.where(corrupt_head.unsqueeze(1), entity_embs[neg_ids], head)
            neg_tails = torch.where(~corrupt_head.unsqueeze(1), entity_embs[neg_ids], tail)
            neg_score = torch.norm(neg_heads + rel - neg_tails, p=2, dim=1)

            # Loss computation
            loss = F.relu(pos_score - neg_score + MARGIN).mean()
            reg_loss = 0.0005 * (entity_embs.norm(2).mean() + rel_embs.norm(2).mean())
            total_loss = loss + reg_loss

            # CRITICAL: Backward pass - computes gradients for ALL parameters
            total_loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # CRITICAL: Optimiser step - updating ALL parameters
            optimizer.step()

            epoch_loss += loss.item()
            num_batches += 1

        scheduler.step()
        avg_loss = epoch_loss / num_batches
        current_lr = scheduler.get_last_lr()[0]
        print(f"  Epoch {epoch:2d}/{epochs} | Loss: {avg_loss:.4f} | LR: {current_lr:.6f}")

    print(" GNN training complete!")
    return model, last_gnn_epoch

# Training GNN
GNN_MODEL, last_gnn_epoch = train_gnn(GNN_MODEL,
                                       SPLIT_DATA['train_edge_index'].to(device),
                                       SPLIT_DATA['train_edge_type'].to(device),
                                       epochs=GNN_EPOCHS)

# ================================
# Extracting Final Embeddings
# ================================
print("\n Extracting final GNN-enhanced embeddings...")
GNN_MODEL.eval()
with torch.no_grad():
    TRAINED_ENTITY_EMBS, TRAINED_RELATION_EMBS = GNN_MODEL(
    SPLIT_DATA['train_edge_index'].to(device),
    SPLIT_DATA['train_edge_type'].to(device))

TRAINED_ENTITY_EMBS = TRAINED_ENTITY_EMBS.cpu().contiguous()
TRAINED_RELATION_EMBS = TRAINED_RELATION_EMBS.cpu().contiguous()

print(f"  • Final entity embeddings: {TRAINED_ENTITY_EMBS.shape}")
print(f"  • Final relation embeddings: {TRAINED_RELATION_EMBS.shape}")
print(f"  • Mean entity norm: {TRAINED_ENTITY_EMBS.norm(dim=1).mean():.3f}")
print(f"  • Mean relation norm: {TRAINED_RELATION_EMBS.norm(dim=1).mean():.3f}")

print("\n Quick validation check:")
sample_size = min(1000, edge_index_dev.shape[1])
sample_idx = torch.randperm(edge_index_dev.shape[1])[:sample_size]
sample_heads = edge_index_dev[0, sample_idx].cpu()
sample_tails = edge_index_dev[1, sample_idx].cpu()
sample_rels  = edge_type_dev[sample_idx].cpu()

with torch.no_grad():
    h = TRAINED_ENTITY_EMBS[sample_heads]
    t = TRAINED_ENTITY_EMBS[sample_tails]
    r = TRAINED_RELATION_EMBS[sample_rels]
    scores = torch.norm(h + r - t, p=2, dim=1)
print(f"  • Mean validation score: {scores.mean():.4f}")
print(f"  • Score std: {scores.std():.4f}")

print("\n" + "="*60)
print(" STEP 4 COMPLETE")
print("="*60)
print("Summary:")
print(f"  1. TransE trained for {last_transe_epoch} epoch(s)")
print(f"  2. GNN trained for {last_gnn_epoch} epoch(s) with TransE init")
print(f"  3. Final embeddings normalized: Entity norm={TRAINED_ENTITY_EMBS.norm(dim=1).mean():.3f}, Relation norm={TRAINED_RELATION_EMBS.norm(dim=1).mean():.3f}")
print("="*60)

# ================================
# STEP 5: NEURAL-GUIDED RETRIEVAL SYSTEM
# ================================
print("\n STEP 5: Neural-Guided Retrieval System")
print("=" * 60)

def set_all_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_all_seeds(42)

# Trying FAISS first; falling back to NumPy if unavailable
try:
    import faiss
    _HAS_FAISS = True
    print("  ✓ FAISS available for accelerated similarity search")
except Exception:
    _HAS_FAISS = False
    print("  ⚠ FAISS not available, using NumPy for similarity search")

def _l2_normalize_rows(x: np.ndarray) -> np.ndarray:
    """L2 normalise rows of a matrix"""
    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-8
    return x / n

def _l2_normalize_vec(v: np.ndarray) -> np.ndarray:
    """L2 normalise a vector"""
    n = np.linalg.norm(v) + 1e-8
    return v / n

class NeuralGuidedPathFinder:
    """
    Neural-guided pathfinding with target handling.
    Uses trained GNN embeddings for semantic path scoring.
    """

    def __init__(self, model, edge_index, edge_type, entity_embeddings, relation_embeddings,
                use_model_path_score: bool = True, beam_width_default: int = 50):
        self.model = model
        self.edge_index = edge_index
        self.edge_type = edge_type
        self.use_model_path_score = use_model_path_score
        self.beam_width_default = beam_width_default

        # Storing embeddings (float32, L2-normalised)
        e = entity_embeddings.detach().cpu().numpy().astype('float32', copy=False)
        r = relation_embeddings.detach().cpu().numpy().astype('float32', copy=False)
        self.entity_embeddings = _l2_normalize_rows(e)
        self.relation_embeddings = _l2_normalize_rows(r)
        self.dim = self.entity_embeddings.shape[1]
        self.num_entities = self.entity_embeddings.shape[0]

        # Building adjacency lists
        self._build_adjacency()

        # Building FAISS index if available
        self._build_faiss_index()

        # Finding hub entities for bridging
        self.well_connected_entities = self._find_well_connected_entities()

        print(f"\n   Neural Path Finder initialised:")
        print(f"    • Entities: {len(self.entity_embeddings):,}")
        print(f"    • Relations: {len(self.relation_embeddings):,}")
        print(f"    • Forward edges: {sum(len(v) for v in self.adjacency.values()):,}")
        print(f"    • Well-connected hubs: {len(self.well_connected_entities)}")

    def _build_adjacency(self):
        """Builds adjacency lists for efficient traversal."""
        self.adjacency = defaultdict(list)
        self.reverse_adjacency = defaultdict(list)

        edge_index = self.edge_index.cpu().numpy()
        edge_type = self.edge_type.cpu().numpy()

        H, T = edge_index
        for i in range(edge_index.shape[1]):
            h, t, r = int(H[i]), int(T[i]), int(edge_type[i])
            self.adjacency[h].append((t, r))
            self.reverse_adjacency[t].append((h, r))

        print(f"    • Built adjacency for {len(self.adjacency)} nodes (outgoing)")
        print(f"    • Built reverse adjacency for {len(self.reverse_adjacency)} nodes (incoming)")

    def _find_well_connected_entities(self, min_in=5, min_out=5, sample_max=2000):
        """Finds entities with both incoming and outgoing edges (good for bridging)."""
        limit = min(sample_max, self.entity_embeddings.shape[0])
        candidates = []

        for nid in range(limit):
            out_edges = len(self.adjacency.get(nid, []))
            in_edges = len(self.reverse_adjacency.get(nid, []))
            if out_edges >= min_out and in_edges >= min_in:
                candidates.append({
                    'id': nid, 'in': in_edges, 'out': out_edges,
                    'total': in_edges + out_edges})

        candidates.sort(key=lambda x: x['total'], reverse=True)
        return [c['id'] for c in candidates[:20]]

    def _build_faiss_index(self):
        """Builds FAISS index for fast similarity search."""
        if not _HAS_FAISS:
            self.faiss_index = None
            return

        xb = self.entity_embeddings.copy()
        faiss.normalize_L2(xb)
        self.faiss_index = faiss.IndexFlatIP(self.dim)
        self.faiss_index.add(xb)
        print(f"    • FAISS index built for {xb.shape[0]} entities")

    def encode_question(self, question: str) -> np.ndarray:
        """Encodes question into embedding space using hashing trick."""
        D = self.dim
        vec = np.zeros(D, dtype='float32')
        q = question.lower().strip()
        tokens = re.findall(r"[a-z0-9_]+", q)

        # Unigrams
        for tok in tokens:
            h = int(hashlib.md5(tok.encode('utf-8')).hexdigest(), 16)
            idx = h % D
            sign = 1.0 if (h >> 1) & 1 else -1.0
            vec[idx] += sign

        # Bigrams for context
        for a, b in zip(tokens, tokens[1:]):
            bg = a + "_" + b
            h = int(hashlib.blake2b(bg.encode('utf-8'), digest_size=8).hexdigest(), 16)
            idx = h % D
            sign = 1.0 if (h >> 2) & 1 else -1.0
            vec[idx] += 0.5 * sign

        # Intent boosts based on question type
        if "how" in q and "connect" in q:
            vec[: D // 4] += 0.25
        if "similar" in q or "related" in q:
            vec[D // 4 : D // 2] += 0.25
        if "what" in q:
            vec[D // 2 : 3 * D // 4] += 0.25

        return _l2_normalize_vec(vec)

    def find_similar_entities(self, entity_id: int, k: int = 10) -> List[Tuple[int, float]]:
        """Finds top-k most similar entities using embeddings."""
        q = self.entity_embeddings[entity_id:entity_id+1]

        if self.faiss_index is not None:
            try:
                scores, indices = self.faiss_index.search(q, k + 1)
                idxs, scs = indices[0], scores[0]
            except Exception:
                # Fallback to NumPy if FAISS fails
                scs = (self.entity_embeddings @ q[0]).astype('float32')
                top = np.argpartition(-scs, min(k+1, len(scs)-1))[:k+1]
                idxs = top[np.argsort(-scs[top])]
                scs = scs[idxs]
        else:
            scs = (self.entity_embeddings @ q[0]).astype('float32')
            top = np.argpartition(-scs, min(k+1, len(scs)-1))[:k+1]
            idxs = top[np.argsort(-scs[top])]
            scs = scs[idxs]

        # Filtering out self and return
        out = []
        for idx, score in zip(idxs, scs):
            if int(idx) != int(entity_id):
                out.append((int(idx), float(score)))
                if len(out) == k:
                    break
        return out

    def shortest_distance(self, start: int, target: int, k_max: int = 6):
        """Returns (directed_distance, undirected_distance) within k_max hops."""
        if start == target:
            return 0, 0

        # Directed BFS
        seen = {start}
        q = deque([(start, 0)])
        d_dir = None

        while q:
            node, d = q.popleft()
            if d >= k_max:
                continue
            for nxt, _ in self.adjacency.get(node, []):
                if nxt == target:
                    d_dir = d + 1
                    q.clear()
                    break
                if nxt not in seen:
                    seen.add(nxt)
                    q.append((nxt, d + 1))

        # Undirected BFS (considers both forward and reverse edges)
        seen = {start}
        q = deque([(start, 0)])
        d_undir = None

        while q:
            node, d = q.popleft()
            if d >= k_max:
                continue

            # Checking forward edges
            for nxt, _ in self.adjacency.get(node, []):
                if nxt == target:
                    d_undir = d + 1
                    q.clear()
                    break
                if nxt not in seen:
                    seen.add(nxt)
                    q.append((nxt, d + 1))

            # Checking reverse edges
            for prv, _ in self.reverse_adjacency.get(node, []):
                if prv == target:
                    d_undir = d + 1
                    q.clear()
                    break
                if prv not in seen:
                    seen.add(prv)
                    q.append((prv, d + 1))

        return d_dir, d_undir

    def neural_heuristic(self, current: int, target: int,
                        question_emb: Optional[np.ndarray] = None,
                        target_weight: float = 0.7) -> float:
        """Heuristic that prioritises target proximity."""
        c = self.entity_embeddings[current]
        t = self.entity_embeddings[target]

        # Cosine distance to target (0 = identical, 2 = opposite)
        cos_dist_target = 1.0 - float(np.dot(c, t))

        # Question relevance
        if question_emb is not None and len(question_emb) == self.dim:
            cos_dist_question = 1.0 - float(np.dot(c, question_emb))
        else:
            cos_dist_question = 1.0

        # Weighted combination
        h = target_weight * cos_dist_target + (1.0 - target_weight) * cos_dist_question
        return max(0.0, h)

    def neural_guided_search(self, start: int, target: int,
                           question_emb: Optional[np.ndarray] = None,
                           max_hops: int = 3, beam_width: int = None,
                           target_weight: float = 0.7) -> List[Tuple[List[int], List[int], float]]:
        """
        A*/beam search that finds paths from start to target.
        Returns list of (path_nodes, path_relations, score).
        """
        if beam_width is None:
            beam_width = self.beam_width_default

        # Priority queue: (f_score, g_cost, path_nodes, path_relations)
        pq = [(0.0, 0, [start], [])]
        visited = set()
        found_paths = []

        while pq and len(found_paths) < 5:
            f_score, g_score, path, rels = heapq.heappop(pq)
            curr = path[-1]
            hops = len(path) - 1

            # Checking if we reached target
            if curr == target and hops > 0:
                score = self.score_path(path, rels, question_emb)
                found_paths.append((path, rels, score))
                continue

            # Skipping if visited or exceeded max hops
            if (curr, hops) in visited or hops >= max_hops:
                continue
            visited.add((curr, hops))

            # Expanding neighbors
            neighbors = []
            for nxt, r in self.adjacency.get(curr, []):
                if nxt in path:  # Avoiding cycles
                    continue

                new_g = g_score + 1
                h = self.neural_heuristic(nxt, target, question_emb, target_weight)
                new_f = new_g + h
                neighbors.append((new_f, new_g, nxt, r))

            # Keeping only top beam_width neighbors
            if neighbors:
                neighbors.sort(key=lambda x: x[0])
                for new_f, new_g, nxt, r in neighbors[:beam_width]:
                    heapq.heappush(pq, (new_f, new_g, path + [nxt], rels + [r]))

        # Sorting by score
        found_paths.sort(key=lambda x: x[2], reverse=True)
        return found_paths

    def validate_path(self, path, rels):
        """Verifies that every forward hop exists in the graph with the correct relation."""
        if len(path) != len(rels) + 1:
            return False

        for i in range(len(path) - 1):
            src, dst = int(path[i]), int(path[i+1])
            rel = int(rels[i])

            # Checking if this exact edge exists
            found = False
            for nxt, r in self.adjacency.get(src, []):
                if nxt == dst and r == rel:
                    found = True
                    break
            if not found:
                return False
        return True

    def find_paths_bidirectional(self, start: int, target: int,
                                question_emb: Optional[np.ndarray] = None,
                                max_hops: int = 4, beam_width: int = None) -> List[Tuple[List[int], List[int], float]]:
        """
        Multiple strategies to find paths:
        1. Forward search (start -> target)
        2. Via hub entities if needed
        Note: Removed backward search as reversing directed paths is not valid
        """
        if beam_width is None:
            beam_width = self.beam_width_default

        all_paths = []

        # Strategy 1: Forward search
        forward_paths = self.neural_guided_search(
            start, target, question_emb, max_hops, beam_width, target_weight=0.8)
        all_paths.extend(forward_paths)

        # Strategy 2: Hub routing disabled - causes suboptimal paths

        # Deduplicating and validating paths
        seen_paths, unique_valid_paths = set(), []
        for path, rels, score in all_paths:
            if len(path) > 1 and path[-1] == target and self.validate_path(path, rels):
                path_key = tuple(path)
                if path_key not in seen_paths:
                    seen_paths.add(path_key)
                    unique_valid_paths.append((path, rels, score))

        # Sorting by score and returning top 5
        unique_valid_paths.sort(key=lambda x: x[2], reverse=True)
        return unique_valid_paths[:5]

    def score_path(self, path: List[int], relations: List[int],
                  question_emb: Optional[np.ndarray] = None) -> float:
        """
        Path scoring balancing length with semantic quality.
        """
        if len(path) < 2:
            return 0.0

        # 1. Moderate length penalty
        path_length = len(path) - 1
        length_score = 1.0 / (1.0 + path_length * 0.8)

        # 2. Path coherence (semantic quality)
        coherence = 0.0
        for i in range(len(path) - 1):
            e1 = self.entity_embeddings[path[i]]
            e2 = self.entity_embeddings[path[i + 1]]
            coherence += max(0.0, float(np.dot(e1, e2)))
        coherence /= max(1, len(path) - 1)

        # 3. Question relevance
        if question_emb is not None and len(question_emb) == self.dim:
            path_vec = np.mean([self.entity_embeddings[e] for e in path], axis=0)
            path_vec = _l2_normalize_vec(path_vec)
            relevance = max(0.0, float(np.dot(path_vec, question_emb)))
        else:
            relevance = 0.5

        # 4. Relation consistency
        if relations and len(set(relations)) == 1:
            consistency_bonus = 0.05
        else:
            consistency_bonus = 0.0

        # Semantic quality emphasised
        base_score = (
            0.30 * length_score +
            0.45 * coherence +
            0.25 * relevance +
            consistency_bonus)

        # Optional: model's learned path importance
        if self.use_model_path_score and hasattr(self.model, "compute_path_importance"):
            try:
                device = next(self.model.parameters()).device
                q_tensor = None
                if question_emb is not None:
                    q_tensor = torch.from_numpy(question_emb).float().to(device)
                model_score = self.model.compute_path_importance(
                    path, relations or [0] * (len(path) - 1), q_tensor)
                final_score = 0.7 * base_score + 0.3 * float(model_score)
            except Exception:
                final_score = base_score
        else:
            final_score = base_score

        return float(np.clip(final_score, 0.01, 1.0))

class AdvancedGraphToText:
    """
    Graph-to-text converter with templates.
    Uses entity and relation names from Step 2.
    """

    def __init__(self, entity_names: Dict[int, str], relation_names: Dict[int, str]):
        self.entity_names = entity_names
        self.relation_names = relation_names
        print(f"\n   Graph-to-Text Converter initialised:")
        print(f"    • Entity vocabulary: {len(entity_names):,}")
        print(f"    • Relation vocabulary: {len(relation_names):,}")

    def path_to_natural_language(self, path: List[int], relations: List[int],
                                score: float = None) -> str:
        """Converts path to natural language description."""
        if len(path) < 2:
            return ""

        # Getting entity names
        ents = []
        for e in path:
            name = self.entity_names.get(int(e), f"Entity_{int(e)}")
            # Truncating very long names for readability
            if len(name) > 40:
                name = name[:37] + "..."
            ents.append(name)

        # Getting relation names
        rels = []
        for r in (relations or []):
            rel_name = self.relation_names.get(int(r), f"R{int(r)}")
            # Making relation names more readable
            if '.' in rel_name:
                # Extracting the most meaningful part
                parts = rel_name.split('.')
                rel_name = parts[-1].replace('_', ' ')
            rels.append(rel_name)

        def rel(i, default="is connected to"):
            return rels[i] if i < len(rels) else default

        # Building description based on path length
        if len(path) == 2:
            text = f"**{ents[0]}** → [{rel(0)}] → **{ents[1]}**"
        elif len(path) == 3:
            text = (f"**{ents[0]}** → [{rel(0)}] → {ents[1]}, "
                   f"which → [{rel(1)}] → **{ents[2]}**")
        elif len(path) == 4:
            text = (f"**{ents[0]}** → [{rel(0)}] → {ents[1]} → "
                   f"[{rel(1)}] → {ents[2]} → [{rel(2)}] → **{ents[3]}**")
        else:
            # For longer paths, using compact notation
            path_str = f"**{ents[0]}**"
            for i in range(1, len(ents)):
                r_name = rel(i-1) if i-1 < len(rels) else "..."
                path_str += f" → [{r_name}] → "
                if i == len(ents) - 1:
                    path_str += f"**{ents[i]}**"
                else:
                    path_str += f"{ents[i]}"
            text = path_str

        # Adding confidence if provided
        if score is not None:
            if score > 0.7:
                confidence_level = "high"
            elif score > 0.4:
                confidence_level = "medium"
            else:
                confidence_level = "low"
            text += f" ({confidence_level}: {score:.2f})"

        return text

    def explain_retrieval(self, retrieval_results: Dict) -> str:
        """Generates comprehensive explanation of retrieval results."""
        lines = []

        # Header with query info
        query_entity = retrieval_results.get('query_entity')
        entity_name = self.entity_names.get(query_entity, f"Entity_{query_entity}")
        lines.append(f"**Retrieval Results for: {entity_name}**")
        lines.append("")

        # Similarity results
        sim = retrieval_results.get('similarity_results') or []
        if sim:
            lines.append("**Similar Entities (by embedding similarity):**")
            for i, (eid, sc) in enumerate(sim[:5], 1):
                nm = self.entity_names.get(int(eid), f"Entity_{int(eid)}")
                if len(nm) > 40:
                    nm = nm[:37] + "..."
                lines.append(f"  {i}. {nm} (similarity: {sc:.3f})")
            lines.append("")

        # Path results
        paths_full = retrieval_results.get('path_results_full') or []
        if paths_full:
            lines.append("**Multi-hop Reasoning Paths:**")
            for i, (p, r, s) in enumerate(paths_full[:3], 1):
                path_text = self.path_to_natural_language(p, r, s)
                lines.append(f"  {i}. {path_text}")
            lines.append("")

        # Confidence assessment
        confidence = retrieval_results.get('confidence', 0.0)
        if confidence > 0.7:
            lines.append("Overall Confidence: High")
        elif confidence > 0.4:
            lines.append("Overall Confidence: Medium")
        else:
            lines.append("Overall Confidence: Low")

        if not sim and not paths_full:
            lines.append("No connections found in the knowledge graph.")

        return "\n".join(lines)

    def format_path_for_llm(self, path: List[int], relations: List[int]) -> str:
        """Formats path in a way that's optimal for LLM consumption."""
        if len(path) < 2:
            return ""

        # Building structured representation
        result = []
        for i in range(len(path) - 1):
            source = self.entity_names.get(path[i], f"Entity_{path[i]}")
            target = self.entity_names.get(path[i+1], f"Entity_{path[i+1]}")

            if i < len(relations):
                rel = self.relation_names.get(relations[i], f"R{relations[i]}")
                result.append(f"({source}) --[{rel}]--> ({target})")
            else:
                result.append(f"({source}) --> ({target})")

        return " | ".join(result)

class HybridRetriever:
    """
    Hybrid retriever with path validation.
    Combines similarity search and multi-hop reasoning.
    """

    def __init__(self, path_finder: NeuralGuidedPathFinder,
                 graph_to_text: AdvancedGraphToText):
        self.path_finder = path_finder
        self.graph_to_text = graph_to_text
        print(f"\n   Hybrid Retriever initialised")

    def _extract_explicit_target(self, question: str, query_entity: int) -> Optional[int]:
        """Extracts explicit target entity from question."""
        if not question:
            return None

        # Finding all entity mentions (handling various formats)
        patterns = [
            r"\bentity\s+(\d+)\b",
            r"\bEntity_(\d+)\b",
            r"\bE(\d+)\b"]

        ids = []
        for pattern in patterns:
            matches = re.findall(pattern, question, re.IGNORECASE)
            ids.extend([int(x) for x in matches])

        # Filtering out the query entity and validating existence
        valid_ids = []
        for eid in ids:
            if eid != int(query_entity) and eid < self.path_finder.num_entities:
                valid_ids.append(eid)

        # Returning first valid different entity if found
        return valid_ids[0] if valid_ids else None

    def retrieve(self, query_entity: int, question: Optional[str] = None,
                top_k: int = 10) -> Dict:
        """
        Retrieval with path finding and validation.
        """
        out = {
            'query_entity': int(query_entity),
            'question': question or "",
            'similarity_results': [],
            'path_results_full': [],
            'path_results': [],
            'confidence': 0.0}

        # 1. Finding similar entities
        out['similarity_results'] = self.path_finder.find_similar_entities(
            query_entity, k=top_k)

        # 2. Extracting target and encoding question
        explicit_target = self._extract_explicit_target(question or "", query_entity)
        q_emb = self.path_finder.encode_question(question) if question else None

        path_results = []

        # 3. If explicit target exists, focusing on finding paths to it
        if explicit_target is not None:
            # Trying to find paths with increasing hop limits
            for max_hops in [3, 4, 5]:
                if len(path_results) >= 3:
                    break

                paths = self.path_finder.find_paths_bidirectional(
                    int(query_entity), int(explicit_target),
                    question_emb=q_emb, max_hops=max_hops,
                    beam_width=60 if max_hops > 3 else 40)

                # Adding valid paths
                for p, r, s in paths:
                    if p[-1] == explicit_target:  # Validating target
                        path_results.append((p, r, s))

                if path_results:
                    break

        # 4. If no explicit target or need more paths, exploring
        if len(path_results) < 3:
            # Using similar entities as exploration targets
            similar_targets = [e for e, _ in out['similarity_results'][:5]]

            for tgt in similar_targets:
                if tgt == int(query_entity):
                    continue
                if explicit_target is not None and tgt == explicit_target:
                    continue

                paths = self.path_finder.neural_guided_search(
                    int(query_entity), int(tgt),
                    question_emb=q_emb, max_hops=3,
                    beam_width=30, target_weight=0.5)

                for p, r, s in paths[:2]:  # Taking top 2 from each
                    if p[-1] == tgt:  # Validation
                        path_results.append((p, r, s * 0.9))  # Slight penalty

                if len(path_results) >= 5:
                    break

        # 5. Re-ranking paths if we have an explicit target
        if explicit_target is not None and path_results:
            boosted = []
            for p, r, s in path_results:
                if p[-1] == explicit_target:
                    # Strong bonus for reaching target
                    new_score = min(0.95, s * 1.3)
                else:
                    # Penalty for not reaching target
                    new_score = s * 0.5
                boosted.append((p, r, new_score))
            path_results = boosted

        # 6. Sorting and taking top results
        path_results.sort(key=lambda x: x[2], reverse=True)
        out['path_results_full'] = path_results[:5]
        out['path_results'] = [(p, s) for (p, r, s) in out['path_results_full']]

        # 7. Calculating confidence
        if out['similarity_results']:
            sim_conf = np.mean([s for _, s in out['similarity_results'][:3]])
        else:
            sim_conf = 0.0

        if out['path_results_full']:
            path_conf = np.mean([s for _, _, s in out['path_results_full'][:3]])
        else:
            path_conf = 0.0

        out['confidence'] = 0.6 * path_conf + 0.4 * sim_conf

        # 8. Generating explanation
        out['explanation'] = self.graph_to_text.explain_retrieval(out)

        return out

# ===== Initialising Retrieval Components =====
print("\n Initialising Retrieval Components...")

# Verifying we have the mappings from Step 2
entity_names = globals().get('ENTITY_NAMES')
relation_names = globals().get('RELATION_NAMES')

if entity_names is None or relation_names is None:
    print("  ⚠ Warning: ENTITY_NAMES or RELATION_NAMES not found from Step 2")
    print("  Creating fallback mappings...")
    if entity_names is None:
        entity_names = {i: f"Entity_{i}" for i in range(NUM_ENTITIES)}
    if relation_names is None:
        relation_names = {i: f"R{i}" for i in range(NUM_RELATIONS)}
else:
    print("   Entity and relation mappings loaded from Step 2")

# Verifying we have trained embeddings from Step 4
trained_entity_embs = globals().get('TRAINED_ENTITY_EMBS')
trained_relation_embs = globals().get('TRAINED_RELATION_EMBS')

if trained_entity_embs is None or trained_relation_embs is None:
    print("  ⚠ Warning: Trained embeddings not found from Step 4")
    print("  Using random embeddings for testing...")
    trained_entity_embs = torch.randn(NUM_ENTITIES, 256)
    trained_relation_embs = torch.randn(NUM_RELATIONS, 256)
else:
    print("   Trained embeddings loaded from Step 4")

# Initialising components
# Using ONLY train edges for path finding
path_finder = NeuralGuidedPathFinder(
    model=GNN_MODEL,
    edge_index=SPLIT_DATA['train_edge_index'],
    edge_type=SPLIT_DATA['train_edge_type'],
    entity_embeddings=trained_entity_embs,
    relation_embeddings=trained_relation_embs,
    use_model_path_score=True,
    beam_width_default=80)

graph_to_text = AdvancedGraphToText(entity_names, relation_names)
retriever = HybridRetriever(path_finder, graph_to_text)

print("\n Retrieval System Initialised Successfully!")

# ===== Testing the System =====
print("\n" + "="*60)
print(" TESTING THE RETRIEVAL SYSTEM")
print("="*60)

# Test Case 1: Known connection
print("\n Test 1: Finding connection between known entities")
print("-" * 40)

test_entity_1 = 32  # United States
test_entity_2 = 434  # Marriage
question_1 = f"How is entity {test_entity_1} connected to entity {test_entity_2}?"

print(f"Query: {question_1}")
print(f"Start: {entity_names.get(test_entity_1)} (ID: {test_entity_1})")
print(f"Target: {entity_names.get(test_entity_2)} (ID: {test_entity_2})")

res1 = retriever.retrieve(test_entity_1, question_1, top_k=5)
print("\n" + res1['explanation'])

# Validating paths
if res1['path_results_full']:
    print("\n Path Validation:")
    for i, (path, rels, score) in enumerate(res1['path_results_full'][:3], 1):
        reaches_target = (path[-1] == test_entity_2)
        status = "PASS" if reaches_target else "FAIL"
        print(f"  Path {i}: [{status}] {'Reaches target' if reaches_target else 'Does not reach target'}")
        print(f"    Hops: {len(path)-1}, Score: {score:.3f}")

# Test Case 2: Similarity search
print("\n Test 2: Finding similar entities")
print("-" * 40)

test_entity_3 = 160  # English language
question_2 = f"What entities are similar to entity {test_entity_3}?"

print(f"Query: {question_2}")
print(f"Entity: {entity_names.get(test_entity_3)} (ID: {test_entity_3})")

res2 = retriever.retrieve(test_entity_3, question_2, top_k=10)
print("\n" + res2['explanation'])

# Test Case 3: Complex multi-hop
print("\n Test 3: Multi-hop reasoning")
print("-" * 40)

if path_finder.well_connected_entities:
    hub1, hub2 = path_finder.well_connected_entities[:2]
    question_3 = f"How is entity {hub1} connected to entity {hub2}?"

    print(f"Query: {question_3}")
    print(f"Start: {entity_names.get(hub1)} (ID: {hub1})")
    print(f"Target: {entity_names.get(hub2)} (ID: {hub2})")

    res3 = retriever.retrieve(hub1, question_3, top_k=5)
    print("\n" + res3['explanation'])

# ===== Validation Functions =====
def validate_retrieval_accuracy(retriever, test_set=None, verbose=False):
    """
    Validates retrieval accuracy on a test set.
    Returns metrics on path-finding success rate.
    """
    if test_set is None:
        # Creating default test set
        test_set = [
            (32, 434, f"How is entity 32 connected to entity 434?"),
            (90, 62, f"How is entity 90 connected to entity 62?"),
            (160, 382, f"How is entity 160 connected to entity 382?")]

    results = {
        'total': len(test_set),
        'paths_found': 0,
        'correct_targets': 0,
        'avg_path_length': [],
        'avg_confidence': []}

    print("\n Running Validation Tests...")
    print("-" * 40)

    for start, target, question in test_set:
        res = retriever.retrieve(start, question, top_k=5)
        paths = res.get('path_results_full', [])

        if paths:
            results['paths_found'] += 1
            results['avg_confidence'].append(res['confidence'])

            # Checking if any path reaches the target
            for path, _, _ in paths:
                if path[-1] == target:
                    results['correct_targets'] += 1
                    results['avg_path_length'].append(len(path) - 1)
                    break

        if verbose:
            status = "PASS" if paths else "FAIL"
            start_name = entity_names.get(start, f"Entity_{start}")[:20]
            target_name = entity_names.get(target, f"Entity_{target}")[:20]
            print(f"  [{status}] {start_name} → {target_name}")
            if paths:
                print(f"      Found {len(paths)} paths, confidence: {res['confidence']:.3f}")

    # Calculating metrics
    success_rate = results['paths_found'] / results['total'] if results['total'] > 0 else 0
    target_accuracy = results['correct_targets'] / results['total'] if results['total'] > 0 else 0
    avg_len = np.mean(results['avg_path_length']) if results['avg_path_length'] else 0
    avg_conf = np.mean(results['avg_confidence']) if results['avg_confidence'] else 0

    print("\n Validation Results:")
    print(f"  • Path Finding Success Rate: {success_rate:.1%}")
    print(f"  • Target Reaching Accuracy: {target_accuracy:.1%}")
    print(f"  • Average Path Length: {avg_len:.2f} hops")
    print(f"  • Average Confidence: {avg_conf:.3f}")

    return results

# Running validation
validation_results = validate_retrieval_accuracy(retriever, verbose=True)

# ===== Graph-to-Text Testing =====
print("\n" + "="*60)
print(" TESTING GRAPH-TO-TEXT")
print("="*60)

# Testing path formatting
test_path = [32, 90, 434]  # United States -> USD -> Marriage
test_rels = [6, 15]  # Some relation IDs

print("\n Test Path Formatting:")
print(f"  Raw path: {test_path}")
print(f"  Raw relations: {test_rels}")

# Natural language
nl_text = graph_to_text.path_to_natural_language(test_path, test_rels, score=0.75)
print(f"\n  Natural Language:")
print(f"    {nl_text}")

# LLM format
llm_text = graph_to_text.format_path_for_llm(test_path, test_rels)
print(f"\n  LLM Format:")
print(f"    {llm_text}")

# ===== Final Quality Check =====
print("\n" + "="*60)
print(" FINAL QUALITY CHECK")
print("="*60)

def final_quality_check():
    """Runs comprehensive quality check on the retrieval system."""

    print("\n1. Entity Name Mapping Check:")
    sample_entities = [32, 90, 160, 434]
    for eid in sample_entities:
        name = entity_names.get(eid, f"Entity_{eid}")
        print(f"   ID {eid}: {name}")

    print("\n2. Relation Name Mapping Check:")
    sample_relations = list(range(min(5, NUM_RELATIONS)))
    for rid in sample_relations:
        name = relation_names.get(rid, f"R{rid}")
        print(f"   ID {rid}: {name}")

    print("\n3. Path Finding Check:")
    test_start = 32
    test_target = 434
    d_dir, d_undir = path_finder.shortest_distance(test_start, test_target, k_max=6)
    print(f"   Distance from {test_start} to {test_target}:")
    print(f"   • Directed: {d_dir if d_dir is not None else 'No path'}")
    print(f"   • Undirected: {d_undir if d_undir is not None else 'No path'}")

    print("\n4. Similarity Search Check:")
    similar = path_finder.find_similar_entities(32, k=3)
    print(f"   Top 3 similar to {entity_names.get(32)}:")
    for eid, score in similar:
        print(f"   • {entity_names.get(eid, f'Entity_{eid}')[:30]}: {score:.3f}")

    print("\n5. Graph-to-Text Check:")
    test_retrieval = {
        'query_entity': 32,
        'similarity_results': [(90, 0.95), (382, 0.92)],
        'path_results_full': [([32, 90, 434], [6, 15], 0.72)],
        'confidence': 0.75}

    explanation = graph_to_text.explain_retrieval(test_retrieval)
    print("   Generated explanation preview:")
    print("   " + explanation.split('\n')[0])

    print("\n All systems operational!")

final_quality_check()

print("\n" + "="*60)
print(" STEP 5 COMPLETED!")
print("="*60)

# Exporting for next steps
RETRIEVER = retriever
PATH_FINDER = path_finder
GRAPH_TO_TEXT = graph_to_text

# ================================
# STEP 6: RAG GENERATION PIPELINE WITH GROQ
# ================================

print("\n STEP 6: RAG Generation Pipeline with Groq")
print("=" * 60)

def set_all_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_all_seeds(42)

# Optional Groq client (fallback to template generation if not available or no key)
try:
    from groq import Groq
    _HAS_GROQ = True
except Exception:
    try:
        import subprocess, sys
        print("Installing groq client...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "groq"])
        from groq import Groq
        _HAS_GROQ = True
    except Exception as e:
        print(f"  Groq client not available ({e}). Will use template-based generation.")
        _HAS_GROQ = False

GROQ_MODEL = "llama-3.1-8b-instant"

class RAGGenerator:
    """
    RAG SYSTEM with:
    • Neural-guided retrieval
    • Adaptive context construction with clear path labeling
    • Confidence-aware generation
    • Multi-evidence aggregation
    • Groq LLM with improved prompting
    • Answer validation and correction
    """

    def __init__(self, retriever, groq_api_key: Optional[str]):
        self.retriever = retriever
        self.use_llm = False
        self.client = None
        self.model_name = GROQ_MODEL

        # ---- LLM init (ONLY llama-3.1-8b-instant) ----
        key = (groq_api_key or "").strip()
        if _HAS_GROQ and key:
            try:
                self.client = Groq(api_key=key)
                # Probing exactly this model
                resp = self.client.chat.completions.create(
                    messages=[{"role": "user", "content": "test"}],
                    model=self.model_name,
                    max_tokens=1,
                    temperature=0)
                _ = resp.choices[0].message.content
                self.use_llm = True
                print(f"   Groq initialised. Using model: {self.model_name}")
            except Exception as e:
                print(f"  Could not use {self.model_name}: {e}")
                print("  Falling back to template generation.")
        else:
            if not key:
                print("  No Groq API key entered. Using template-based generation.")
            else:
                print("  Groq client unavailable. Using template-based generation.")

    def construct_adaptive_context(self, retrieval_results: Dict, max_length: int = 1400) -> tuple:
        """Builds crystal-clear context with explicit path labeling."""
        parts = []
        total_conf, n = 0.0, 0

        # Global provenance emphasis
        parts.append("[IMPORTANT: The following are ACTUAL connections from a knowledge graph, not general knowledge]")

        qid = retrieval_results.get('query_entity', 'Unknown')
        qname = self.retriever.graph_to_text.entity_names.get(qid, f"Entity_{qid}")

        parts.append(f"Query Entity: {qname} (ID: {qid})")

        # Similar entities
        sims = retrieval_results.get('similarity_results') or []
        sims = [(e, s) for (e, s) in sims if s > 0.5][:5]
        if sims:
            parts.append("\n[Similar Entities]")
            for e, s in sims:
                name = self.retriever.graph_to_text.entity_names.get(e, f"Entity_{e}")
                parts.append(f"• {name} (similarity: {s:.3f})")
                total_conf += float(s); n += 1

        # Paths - ULTRA CLEAR LABELLING
        paths = retrieval_results.get('path_results_full') or []
        paths = [(p, r, sc) for (p, r, sc) in paths if sc > 0.10][:5]

        if paths:
            parts.append("\n[Knowledge Graph Paths Found]")
            parts.append("(These are ACTUAL CONNECTIONS in the graph)")

            for i, (path, rels, sc) in enumerate(paths, 1):
                ents = []
                for pid in path:
                    pid = int(pid)
                    ents.append(self.retriever.graph_to_text.entity_names.get(pid, f"Entity_{pid}"))

                # Per-path provenance emphasis
                parts.append(f"\n{i}. VERIFIED GRAPH PATH (confidence: {sc:.2f}):")
                parts.append("   This is a real connection in our knowledge base, not an inference.")

                path_str = " → ".join(ents)
                hop_count = len(path) - 1

                # EXPLICIT path type labeling
                if hop_count == 1:
                    parts.append(f"\n{i}. **DIRECT CONNECTION** (1 hop):")
                    parts.append(f"   {ents[0]} connects directly to {ents[1]}")
                    parts.append(f"   Full path: {path_str}")
                elif hop_count == 2:
                    parts.append(f"\n{i}. **2-HOP PATH** (indirect):")
                    parts.append(f"   {ents[0]} → {ents[1]} → {ents[2]}")
                    parts.append(f"   (Goes through intermediate entity: {ents[1]})")
                elif hop_count == 3:
                    parts.append(f"\n{i}. **3-HOP PATH** (multi-step):")
                    parts.append(f"   {path_str}")
                    parts.append(f"   (Goes through: {', '.join(ents[1:-1])})")
                else:
                    parts.append(f"\n{i}. **{hop_count}-HOP PATH**:")
                    parts.append(f"   {path_str}")

                # Adding relation details if available
                if rels and len(rels) > 0:
                    rel_names = []
                    for r in rels:
                        rel_names.append(self.retriever.graph_to_text.relation_names.get(r, f"rel_{r}"))
                    if len(rel_names) <= 3:
                        parts.append(f"   Relations: {' → '.join(rel_names)}")

                parts.append(f"   Confidence: {sc:.3f}")
                total_conf += float(sc); n += 1

        # Graph stats
        adj = self.retriever.path_finder.adjacency
        radj = self.retriever.path_finder.reverse_adjacency

        parts.append("\n[Graph Statistics]")
        parts.append(f"• Query entity has {len(adj.get(qid, []))} outgoing connections")
        parts.append(f"• Query entity has {len(radj.get(qid, []))} incoming connections")

        avg_conf = total_conf / max(1, n)
        parts.append(f"\n[Overall Confidence: {avg_conf:.2f}]")

        ctx = "\n".join(parts)
        if len(ctx) > max_length:
            ctx = ctx[:max_length] + "\n... [truncated for length]"

        return ctx, avg_conf

    def validate_and_correct_answer(self, answer: str, context: str, question: str) -> str:
        """Validates and corrects common LLM misinterpretations."""
        answer_lower = answer.lower()
        context_lower = context.lower()

        # If context shows DIRECT CONNECTION but answer says no direct connection
        if "**direct connection**" in context_lower and "no direct connection" in answer_lower:
            # Extracting the direct connection from context
            lines = context.split('\n')
            for i, line in enumerate(lines):
                if "**DIRECT CONNECTION**" in line:
                    # Getting the next line which has the connection
                    for j in range(i+1, min(i+3, len(lines))):
                        if "connects directly to" in lines[j]:
                            connection = lines[j].strip()
                            answer = f"There is a direct connection: {connection}"
                            break
                    break

        # If answer claims no connection but paths exist
        if ("no connection" in answer_lower or "not connected" in answer_lower) and "[Knowledge Graph Paths Found]" in context:
            # Counting paths in context
            num_paths = context.count("HOP PATH")
            if num_paths > 0:
                answer = answer.replace("no connection", f"{num_paths} connection path(s) found")
                answer = answer.replace("not connected", f"connected through {num_paths} path(s)")

        # Ensuring answer addresses the question
        if "how" in question.lower() and "connected" in question.lower():
            if "HOP PATH" in context and "HOP" not in answer:
                # Answer doesn't mention the path type, add it
                if "**DIRECT CONNECTION**" in context:
                    answer += " (This is a direct 1-hop connection.)"
                elif "**2-HOP PATH**" in context:
                    answer += " (This is a 2-hop connection through an intermediate entity.)"

        return answer

    def generate_with_groq(self, question: str, context: str, confidence: float) -> str:
        """Groq generation with prompting and validation."""
        if not self.use_llm or not self.client:
            return self._template_generation(question, context, confidence)

        try:
            # Parsing context to understand what we found
            has_direct = "**DIRECT CONNECTION**" in context
            has_paths = "[Knowledge Graph Paths Found]" in context
            has_similarity = "[Similar Entities]" in context

            # Counting paths
            num_paths = context.count("HOP PATH") + context.count("DIRECT CONNECTION")

            # System prompt with clear instructions
            system_prompt = (
                "You are a knowledge graph expert assistant. Your role is to accurately describe "
                "the connections and relationships found in the knowledge graph.\n\n"
                "CRITICAL RULES:\n"
                "1. If '**DIRECT CONNECTION**' appears, this means there IS a direct 1-hop connection\n"
                "2. If '**2-HOP PATH**' appears, describe it as an indirect connection through one intermediate entity\n"
                "3. If '**3-HOP PATH**' appears, describe it as a multi-step connection\n"
                "4. NEVER say 'no connection' or 'not connected' if paths are shown\n"
                "5. Always describe the ACTUAL path shown in the context\n"
                "6. Be concise but accurate (2-3 sentences maximum)\n"
                "7. If multiple paths exist, mention the shortest/most direct one first")

            # Building focused user prompt
            if has_paths:
                user_prompt = (
                    f"Based on the knowledge graph analysis below, answer this question:\n"
                    f"Question: {question}\n\n"
                    f"Context (found {num_paths} path(s)):\n{context}\n\n"
                    f"Instructions: Describe the connection(s) found. "
                    f"If there's a DIRECT CONNECTION, state it clearly as a direct 1-hop connection. "
                    f"For multi-hop paths, briefly explain the route.")
            else:
                user_prompt = (
                    f"Question: {question}\n\n"
                    f"Context:\n{context}\n\n"
                    f"Answer based on what was found in the knowledge graph.")

            # Generating response with low temperature for consistency
            resp = self.client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                model=self.model_name,
                temperature=0.1,  # Low for consistency
                max_tokens=250,
                top_p=0.9)

            answer = (resp.choices[0].message.content or "").strip()

            # Validating and correcting the answer
            answer = self.validate_and_correct_answer(answer, context, question)

            return answer

        except Exception as e:
            print(f"  Groq API error: {e}. Falling back to template generation.")
            return self._template_generation(question, context, confidence)

    def _template_generation(self, question: str, context: str, confidence: float) -> str:
        """Template-based fallback generation."""
        q = question.lower()
        lines = context.splitlines()

        # Checking what we found
        has_direct = "**DIRECT CONNECTION**" in context
        has_2hop = "**2-HOP PATH**" in context
        has_3hop = "**3-HOP PATH**" in context

        # Extracting paths more carefully
        paths = []
        for i, line in enumerate(lines):
            if "Full path:" in line or "→" in line:
                path_text = line.strip()
                if "Full path:" in path_text:
                    path_text = path_text.split("Full path:")[-1].strip()
                paths.append(path_text)

        # Extracting similar entities
        sims = [ln.split("•", 1)[-1].strip() for ln in lines if "similarity:" in ln][:3]

        # Generating appropriate response
        if "how" in q and "connect" in q:
            if has_direct:
                # Finding the direct connection details
                for i, line in enumerate(lines):
                    if "connects directly to" in line:
                        return f"Direct connection found: {line.strip()}. This is a 1-hop direct connection."
                return f"There is a direct 1-hop connection. {paths[0] if paths else ''}"
            elif has_2hop:
                return f"Connected via 2-hop path: {paths[0] if paths else 'through an intermediate entity'}."
            elif has_3hop:
                return f"Connected via 3-hop path: {paths[0] if paths else 'through multiple intermediate entities'}."
            elif paths:
                return f"Connection found: {paths[0]}. Confidence: {confidence:.2f}."
            return "No connection path found within 3 hops in the knowledge graph."

        if "similar" in q or "related" in q:
            if sims:
                return f"Most similar entities: {', '.join(sims[:3])}."
            return "No highly similar entities found in the embedding space."

        if "what" in q and "connection" in q:
            if paths:
                response = f"Found {len(paths)} connection(s): "
                if has_direct:
                    response += "including a direct connection. "
                response += f"Shortest: {paths[0]}."
                return response
            return "This entity has limited connections in the current graph view."

        # Generic fallback
        if has_direct:
            return f"Direct connection exists: {paths[0] if paths else 'see context'}."
        elif paths:
            return f"Connection path: {paths[0]}."
        elif sims:
            return f"Related entities include: {', '.join(sims[:2])}."

        return "Limited evidence found in the knowledge graph for this query."

    def generate(self, entity_id: int, question: str) -> Dict:
        """End-to-end pipeline: retrieve → context → generate → validate."""
        t0 = time.time()

        print(f"\n{'='*60}")
        print(f"Processing Query: '{question}'")
        print(f"Starting Entity: {entity_id} ({self.retriever.graph_to_text.entity_names.get(entity_id, f'Entity_{entity_id}')})")

        # Step 1: Retrieval
        print("  → Retrieving from knowledge graph...")
        retrieval = self.retriever.retrieve(entity_id, question, top_k=5)

        # Step 2: Context Construction
        print("  → Constructing context...")
        context, avg_conf = self.construct_adaptive_context(retrieval)

        # Adding diagnostics if we have an explicit target
        try:
            explicit_target = self.retriever._extract_explicit_target(question, entity_id)
        except Exception:
            explicit_target = None

        if explicit_target is not None:
            d_dir, d_undir = self.retriever.path_finder.shortest_distance(
                entity_id, explicit_target, k_max=6)
            context += (
                f"\n\n[Distance Analysis]"
                f"\n• Shortest directed distance (≤6 hops): {d_dir if d_dir else 'Not found'}"
                f"\n• Shortest undirected distance (≤6 hops): {d_undir if d_undir else 'Not found'}")

        # Step 3: Generation
        print(f"  → Generating answer{' with Groq...' if self.use_llm else ' (template)...'}")
        answer = self.generate_with_groq(question, context, avg_conf)

        # Step 4: Metrics
        metrics = {
            'retrieval_time': time.time() - t0,
            'context_length': len(context),
            'answer_length': len(answer.split()),
            'num_paths': len(retrieval.get('path_results_full', [])),
            'num_similar': len(retrieval.get('similarity_results', [])),
            'confidence': avg_conf}

        print(f"   Complete in {metrics['retrieval_time']:.2f}s")

        return {
            'question': question,
            'answer': answer,
            'confidence': avg_conf,
            'context': context,
            'retrieval_results': retrieval,
            'metrics': metrics}

# ================================
# EVALUATION
# ================================

class RAGEvaluator:
    """Evaluator for the RAG system with quality metrics."""

    def __init__(self, rag_system: RAGGenerator):
        self.rag_system = rag_system
        self.results = []

    def evaluate_single(self, entity_id: int, question: str) -> Dict:
        """Evaluates a single question."""
        res = self.rag_system.generate(entity_id, question)

        # Assessing answer quality
        res['evaluation'] = {
            'has_paths': bool(res['retrieval_results'].get('path_results_full')),
            'has_similar': bool(res['retrieval_results'].get('similarity_results')),
            'confidence_level': (
                'high' if res['confidence'] > 0.7
                else 'medium' if res['confidence'] > 0.4
                else 'low'),
            'response_quality': self._assess_quality(res['answer'], res['context'], res['confidence']),
            'answers_question': self._check_answer_relevance(res['answer'], res['question'])}

        self.results.append(res)
        return res

    @staticmethod
    def _assess_quality(answer: str, context: str, confidence: float) -> str:
        """Assesses the quality of an answer."""
        n = len(answer.split())

        # Checking for key quality indicators
        has_path_description = any(word in answer.lower() for word in ['direct', 'hop', 'connection', 'path'])
        has_specific_entities = '→' in answer or 'connects' in answer.lower()

        if n < 5:
            return "too_short"
        if n > 100:
            return "too_long"

        # Quality based on content and confidence
        if confidence > 0.7 and has_path_description:
            return "excellent" if has_specific_entities else "good"
        elif confidence > 0.4 and (has_path_description or n >= 10):
            return "good"
        elif confidence > 0.2:
            return "acceptable"

        return "poor"

    @staticmethod
    def _check_answer_relevance(answer: str, question: str) -> bool:
        """Checks if answer addresses the question."""
        q_lower = question.lower()
        a_lower = answer.lower()

        if "how" in q_lower and "connect" in q_lower:
            return any(word in a_lower for word in ['connection', 'connected', 'path', 'hop', 'direct'])

        if "similar" in q_lower or "related" in q_lower:
            return any(word in a_lower for word in ['similar', 'related', 'entities', 'similarity'])

        return True  # Default to relevant

    def print_summary(self):
        """Prints evaluation summary."""
        if not self.results:
            print("No results to summarize.")
            return

        print("\n" + "="*60)
        print("EVALUATION SUMMARY")
        print("="*60)

        n = len(self.results)
        avg_conf = sum(r['confidence'] for r in self.results) / n
        avg_time = sum(r['metrics']['retrieval_time'] for r in self.results) / n
        avg_len = sum(r['metrics']['answer_length'] for r in self.results) / n

        print(f"Total Questions: {n}")
        print(f"Average Confidence: {avg_conf:.3f}")
        print(f"Average Response Time: {avg_time:.2f}s")
        print(f"Average Answer Length: {avg_len:.1f} words")

        # Quality distribution
        qd = {}
        for r in self.results:
            q = r['evaluation']['response_quality']
            qd[q] = qd.get(q, 0) + 1

        print("\nResponse Quality Distribution:")
        for k in ['excellent', 'good', 'acceptable', 'poor', 'too_short', 'too_long']:
            if k in qd:
                print(f"  {k}: {qd[k]}/{n} ({100.0*qd[k]/n:.1f}%)")

        # Relevance check
        relevant = sum(1 for r in self.results if r['evaluation']['answers_question'])
        print(f"\nAnswer Relevance: {relevant}/{n} ({100.0*relevant/n:.1f}%)")

# ================================
# INITIALISATION AND TESTING
# ================================

# Prompt for API key
api_key_input = getpass.getpass("Enter GROQ API key for llama-3.1-8b-instant: ").strip()

print("\n Initialising RAG System...")
rag_system = RAGGenerator(RETRIEVER, groq_api_key=(api_key_input if api_key_input else None))
evaluator = RAGEvaluator(rag_system)

print("\n" + "="*60)
print("TESTING RAG SYSTEM")
print("="*60)

# Creating comprehensive test cases
test_entities = PATH_FINDER.well_connected_entities[:5] if PATH_FINDER.well_connected_entities else [32, 434, 160]

test_questions = [
    (test_entities[0], f"How is entity {test_entities[0]} connected to entity {test_entities[1]}?"),
    (test_entities[1], f"What entities are most similar to entity {test_entities[1]}?"),
    (32, "How is entity 32 connected to entity 434?"),  # Known test case
]

# Adding more diverse test cases if we have entities
if len(test_entities) > 2:
    test_questions.extend([
        (test_entities[2], f"What are the main connections of entity {test_entities[2]}?"),
        (test_entities[0], f"How is entity {test_entities[0]} connected to entity {test_entities[3]}?")])

# Running evaluations
for entity_id, question in test_questions:
    result = evaluator.evaluate_single(entity_id, question)

    print(f"\n{'='*60}")
    print(f"Question: {question}")
    print(f"Entity: {ENTITY_NAMES.get(entity_id, f'Entity_{entity_id}')} (ID: {entity_id})")
    print(f"Confidence: {result['confidence']:.3f}")
    print(f"Quality: {result['evaluation']['response_quality']}")
    print(f"Relevant: {'Yes' if result['evaluation']['answers_question'] else 'No'}")
    print(f"\nAnswer:")
    print(result['answer'])

    # Showing key metrics
    print(f"\nMetrics:")
    print(f"  • Paths found: {result['metrics']['num_paths']}")
    print(f"  • Similar entities: {result['metrics']['num_similar']}")
    print(f"  • Response time: {result['metrics']['retrieval_time']:.3f}s")
    print(f"  • Answer length: {result['metrics']['answer_length']} words")

# Printing summary
evaluator.print_summary()

print("\n" + "="*60)
print(" STEP 6 COMPLETE - RAG System Ready!")
print("="*60)

# ================================
# STEP 7: LINK PREDICTION EVALUATION (STANDARD KG BENCHMARK)
# ================================

print("\n" + "="*60)
print(" STEP 7: Link Prediction Evaluation")
print("="*60)

def set_all_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_all_seeds(42)

# ================================
# PREPARING TEST DATA
# ================================

print("\n Preparing test triples...")

test_edge_index = SPLIT_DATA['test_edge_index'].cpu()
test_edge_type = SPLIT_DATA['test_edge_type'].cpu()
num_test = test_edge_index.shape[1]

print(f"  • Test triples: {num_test:,}")
print(f"  • Entities: {NUM_ENTITIES:,}")
print(f"  • Relations: {NUM_RELATIONS}")

# Sample for faster evaluation for now (Ideally, to evaluate on the full test set)
MAX_TEST_SAMPLES = 3000
if num_test > MAX_TEST_SAMPLES:
    sample_idx = torch.randperm(num_test)[:MAX_TEST_SAMPLES]
    test_edge_index = test_edge_index[:, sample_idx]
    test_edge_type = test_edge_type[sample_idx]
    num_test = MAX_TEST_SAMPLES
    print(f"  • Sampled down to: {num_test:,} for speed")

# ================================
# LINK PREDICTION FUNCTIONS
# ================================

def score_all_candidates(model, head, relation, edge_index, edge_type):
    """
    Scores all possible tail entities for (head, relation, ?).
    Returns: scores for each entity [NUM_ENTITIES]
    """
    model.eval()
    with torch.no_grad():
        # Getting embeddings from trained model
        entity_embs, relation_embs = model(edge_index.to(device), edge_type.to(device))

        # TransE scoring: ||h + r - t||
        h = entity_embs[head]  # [D]
        r = relation_embs[relation]  # [D]
        all_t = entity_embs  # [NUM_ENTITIES, D]

        # Computing distances
        scores = torch.norm(h + r - all_t, p=2, dim=1)  # [NUM_ENTITIES]

        # Negating so higher is better (for ranking)
        scores = -scores

    return scores.cpu()

def filter_scores(scores, head, relation, test_triple_set, entity_id):
    """
    Filters out other true triples (standard filtered setting).
    """
    filtered_scores = scores.clone()

    for tail in range(len(scores)):
        if tail == entity_id:
            continue  # Keeping the target
        if (int(head), int(relation), int(tail)) in test_triple_set:
            filtered_scores[tail] = float('-inf')  # Masking out other true answers

    return filtered_scores

def compute_rank(scores, true_tail):
    """
    Computes rank of true tail among all candidates.
    Rank 1 = best (true tail scored highest)
    """
    sorted_indices = torch.argsort(scores, descending=True)
    rank = (sorted_indices == true_tail).nonzero(as_tuple=True)[0].item() + 1
    return rank

# ================================
# BUILDING TEST TRIPLE SET
# ================================

print("\n Building test triple set for filtering...")
test_triple_set = set()
for i in range(test_edge_index.shape[1]):
    h = int(test_edge_index[0, i])
    t = int(test_edge_index[1, i])
    r = int(test_edge_type[i])
    test_triple_set.add((h, r, t))

print(f"  • Test triple set size: {len(test_triple_set):,}")

# ================================
# RUNNING EVALUATION
# ================================

print("\n Running link prediction evaluation...")
print("  (This evaluates: given (head, relation), predict tail)")

ranks = []
reciprocal_ranks = []

# Using train edges for scoring (model sees these during forward pass)
train_edge_index = SPLIT_DATA['train_edge_index']
train_edge_type = SPLIT_DATA['train_edge_type']

for i in tqdm(range(num_test), desc="Evaluating"):
    head = int(test_edge_index[0, i])
    tail = int(test_edge_index[1, i])
    relation = int(test_edge_type[i])

    # Scoring all possible tails
    scores = score_all_candidates(
        GNN_MODEL, head, relation,
        train_edge_index, train_edge_type)

    # Filtering out other true triples (standard filtered setting)
    filtered_scores = filter_scores(scores, head, relation, test_triple_set, tail)

    # Computing rank of true tail
    rank = compute_rank(filtered_scores, tail)
    ranks.append(rank)
    reciprocal_ranks.append(1.0 / rank)

# ================================
# COMPUTING METRICS
# ================================

print("\n" + "="*60)
print(" LINK PREDICTION RESULTS")
print("="*60)

ranks = np.array(ranks)
reciprocal_ranks = np.array(reciprocal_ranks)

# Mean Reciprocal Rank
mrr = np.mean(reciprocal_ranks)

# Hits@K
hits_at_1 = np.mean(ranks <= 1) * 100
hits_at_3 = np.mean(ranks <= 3) * 100
hits_at_10 = np.mean(ranks <= 10) * 100

# Mean Rank
mean_rank = np.mean(ranks)

print(f"\nGNN-RAG Link Prediction Performance:")
print(f"  • MRR (Mean Reciprocal Rank): {mrr:.4f}")
print(f"  • Hits@1: {hits_at_1:.2f}%")
print(f"  • Hits@3: {hits_at_3:.2f}%")
print(f"  • Hits@10: {hits_at_10:.2f}%")
print(f"  • Mean Rank: {mean_rank:.1f}")

print(f"\nInterpretation:")
print(f"  • MRR {mrr:.4f} means on average, the true answer ranks in top ~{int(1/mrr)} positions")
print(f"  • Hits@10 {hits_at_10:.1f}% means the true answer is in top-10 for {hits_at_10:.1f}% of queries")

# ================================
# COMPARISON WITH BASELINES
# ================================

print("\n" + "="*60)
print(" BASELINE COMPARISON")
print("="*60)

# TransE-only baseline (no GNN)
print("\n Evaluating TransE-only baseline...")

def score_all_candidates_transe(entity_embs, relation_embs, head, relation):
    """TransE scoring without GNN"""
    h = entity_embs[head]
    r = relation_embs[relation]
    all_t = entity_embs
    scores = torch.norm(h + r - all_t, p=2, dim=1)
    return -scores

# Using the TransE embeddings from Step 4 (before GNN training)
transe_entity_embs = transe_entity_embs_proj  # From Step 4
transe_relation_embs = transe_relation_embs_proj

ranks_transe = []
reciprocal_ranks_transe = []

for i in tqdm(range(num_test), desc="TransE baseline"):
    head = int(test_edge_index[0, i])
    tail = int(test_edge_index[1, i])
    relation = int(test_edge_type[i])

    scores = score_all_candidates_transe(
        transe_entity_embs, transe_relation_embs, head, relation)

    filtered_scores = filter_scores(scores, head, relation, test_triple_set, tail)
    rank = compute_rank(filtered_scores, tail)

    ranks_transe.append(rank)
    reciprocal_ranks_transe.append(1.0 / rank)

ranks_transe = np.array(ranks_transe)
reciprocal_ranks_transe = np.array(reciprocal_ranks_transe)

mrr_transe = np.mean(reciprocal_ranks_transe)
hits_at_10_transe = np.mean(ranks_transe <= 10) * 100
mean_rank_transe = np.mean(ranks_transe)

print(f"\nTransE-only (no GNN) Performance:")
print(f"  • MRR: {mrr_transe:.4f}")
print(f"  • Hits@10: {hits_at_10_transe:.2f}%")
print(f"  • Mean Rank: {mean_rank_transe:.1f}")

print(f"\n Improvement from GNN:")
print(f"  • MRR: {mrr:.4f} vs {mrr_transe:.4f} ({'+' if mrr > mrr_transe else ''}{((mrr - mrr_transe) / mrr_transe * 100):.1f}%)")
print(f"  • Hits@10: {hits_at_10:.1f}% vs {hits_at_10_transe:.1f}% ({'+' if hits_at_10 > hits_at_10_transe else ''}{(hits_at_10 - hits_at_10_transe):.1f}%)")
print(f"  • Mean Rank: {mean_rank:.1f} vs {mean_rank_transe:.1f} ({'better' if mean_rank < mean_rank_transe else 'worse'})")

# ================================
# STATISTICAL SIGNIFICANCE
# ================================

print("\n" + "="*60)
print(" STATISTICAL SIGNIFICANCE")
print("="*60)

from scipy.stats import wilcoxon

# Wilcoxon signed-rank test (paired, non-parametric)
stat, p_value = wilcoxon(reciprocal_ranks, reciprocal_ranks_transe, alternative='greater')

print(f"\nWilcoxon test (GNN-RAG vs TransE-only):")
print(f"  • p-value: {p_value:.6f}")
print(f"  • Result: {'Significant improvement (p<0.05)' if p_value < 0.05 else 'Not significant'}")

# ================================
# VISUALISATION
# ================================

print("\n Generating visualisation...")

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# 1. MRR Comparison
ax1 = axes[0]
methods = ['GNN-RAG\n(Ours)', 'TransE-only']
mrrs = [mrr, mrr_transe]
bars = ax1.bar(methods, mrrs, color=['#2E7D32', '#D32F2F'])
ax1.set_ylabel('MRR', fontsize=12)
ax1.set_title('Mean Reciprocal Rank\n(Higher is Better)', fontsize=14, fontweight='bold')
ax1.set_ylim([0, max(mrrs) * 1.2])
for bar, val in zip(bars, mrrs):
    ax1.text(bar.get_x() + bar.get_width()/2, val + max(mrrs)*0.02,
             f'{val:.4f}', ha='center', fontweight='bold')

# 2. Hits@K Comparison
ax2 = axes[1]
x = np.arange(3)
width = 0.35
hits_gnn = [hits_at_1, hits_at_3, hits_at_10]
hits_transe = [np.mean(ranks_transe <= 1)*100,
               np.mean(ranks_transe <= 3)*100,
               hits_at_10_transe]
ax2.bar(x - width/2, hits_gnn, width, label='GNN-RAG', color='#2E7D32')
ax2.bar(x + width/2, hits_transe, width, label='TransE-only', color='#D32F2F')
ax2.set_ylabel('Percentage (%)', fontsize=12)
ax2.set_title('Hits@K\n(Higher is Better)', fontsize=14, fontweight='bold')
ax2.set_xticks(x)
ax2.set_xticklabels(['Hits@1', 'Hits@3', 'Hits@10'])
ax2.legend()
ax2.set_ylim([0, 110])

# 3. Rank Distribution
ax3 = axes[2]
bins = [1, 2, 3, 5, 10, 20, 50, 100, NUM_ENTITIES]
ax3.hist([ranks, ranks_transe], bins=bins, label=['GNN-RAG', 'TransE-only'],
         color=['#2E7D32', '#D32F2F'], alpha=0.7)
ax3.set_xlabel('Rank of True Answer', fontsize=12)
ax3.set_ylabel('Frequency', fontsize=12)
ax3.set_title('Rank Distribution\n(Left is Better)', fontsize=14, fontweight='bold')
ax3.set_xscale('log')
ax3.legend()
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('link_prediction_evaluation.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n Visualisation saved to 'link_prediction_evaluation.png'")

# ================================
# FINAL SUMMARY
# ================================

print("\n" + "="*60)
print(" FINAL SUMMARY")
print("="*60)

print(f"\nThis evaluation tests generalisation to unseen edges:")
print(f"  • Model trained on {SPLIT_DATA['train_edge_index'].shape[1]:,} edges")
print(f"  • Evaluated on {num_test:,} held-out test edges")
print(f"  • Results show {'significant' if p_value < 0.05 else 'no significant'} improvement from GNN over TransE")

if mrr > mrr_transe:
    print(f"\n✓ GNN improves link prediction by {((mrr - mrr_transe) / mrr_transe * 100):.1f}% MRR")
    print(f"  This demonstrates the GNN learns useful structural patterns beyond TransE")
else:
    print(f"\n⚠ GNN did not improve over TransE baseline")
    print(f"  Consider: longer training, different architecture, or hyperparameter tuning")

print("\n" + "="*60)
print(" STEP 7 COMPLETE")
print("="*60)

# ================================
# STEP 8: REASONING BENCHMARK EVALUATION (USING STEP 2 QUESTIONS)
# ================================

print("\n" + "="*60)
print(" STEP 8: Reasoning Benchmark Evaluation")
print("="*60)

print("\nThis step uses the benchmark questions generated in Step 2 to evaluate")
print("multi-hop reasoning and path-finding capabilities on held-out test edges.")

# ================================
# INSTANTIATING TEST CASES FROM BENCHMARK QUESTIONS
# ================================

print("\n Instantiating test cases from benchmark questions...")

def compute_distance_bfs(source, target, adjacency, max_hops=5):
    """Computes shortest path distance via BFS."""
    if source == target:
        return 0

    visited = {source}
    queue = [(source, 0)]

    while queue:
        node, dist = queue.pop(0)
        if dist >= max_hops:
            continue

        for neighbor, _ in adjacency.get(node, []):
            if neighbor == target:
                return dist + 1
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append((neighbor, dist + 1))

    return None

def instantiate_multihop_cases(benchmark, adjacency, max_per_pattern=3):
    """Creates actual test cases from relation patterns, using only test edges."""
    instantiated = []

    for q in benchmark:
        qtype = q['type']

        if qtype == '1-hop':
            # Already have source/target from benchmark
            instantiated.append({
                'source': q['source'],
                'target': q['target'],
                'true_distance': 1,
                'difficulty': 'easy',
                'type': qtype,
                'question': q['question_template']})

        elif qtype in ['2-hop', '3-hop']:
            pattern = q.get('pattern')
            if not pattern:
                continue

            # Finding actual entity pairs in test adjacency that follow this pattern
            cases_found = 0
            sample_nodes = random.sample(list(adjacency.keys()), min(500, len(adjacency)))

            for source in sample_nodes:
                if cases_found >= max_per_pattern:
                    break

                # Traversing the pattern
                current_nodes = {source}

                for rel_id in pattern:
                    next_nodes = set()
                    for node in current_nodes:
                        for neighbor, r in adjacency.get(node, []):
                            if r == rel_id:
                                next_nodes.add(neighbor)
                    current_nodes = next_nodes
                    if not current_nodes:
                        break

                # If we found targets following this pattern
                if current_nodes:
                    for target in list(current_nodes)[:1]:  # Taking one target per source
                        instantiated.append({
                            'source': source,
                            'target': target,
                            'true_distance': len(pattern),
                            'difficulty': 'medium' if len(pattern) == 2 else 'hard',
                            'type': qtype,
                            'question': f"How is entity {source} connected to entity {target}?",
                            'pattern': pattern})
                        cases_found += 1
                        break

        elif qtype == 'similarity':
            # For similarity, creating cases to find similar entities
            instantiated.append({
                'source': q['source'],
                'target': None,  # No fixed target; eval top-k similarity
                'true_distance': None,
                'difficulty': 'medium',
                'type': qtype,
                'question': q['question_template']})

        elif qtype == 'neighborhood':
            # For neighbourhood, eval retrieval of local graph
            instantiated.append({
                'source': q['source'],
                'target': None,
                'true_distance': None,
                'difficulty': 'easy',
                'type': qtype,
                'question': q['question_template']})

    # Adding negative cases for robustness (path non-existence)
    num_negatives = len(instantiated) // 4
    all_entities = list(range(data.num_nodes))
    for _ in range(num_negatives):
        source = random.choice(all_entities)
        target = random.choice(all_entities)
        while source == target or compute_distance_bfs(source, target, adjacency, max_hops=3) is not None:
            target = random.choice(all_entities)
        instantiated.append({
            'source': source,
            'target': target,
            'true_distance': float('inf'),
            'difficulty': 'negative',
            'type': 'path-existence',
            'question': f"Is there a path between entity {source} and {target}?"})

    random.shuffle(instantiated)  # Shuffling for unbiased eval
    return instantiated

# Building test adjacency from test edges only (no leakage)
test_adjacency = defaultdict(list)
test_reverse_adjacency = defaultdict(list)
test_edge_index = SPLIT_DATA['test_edge_index'].cpu().numpy()
test_edge_type = SPLIT_DATA['test_edge_type'].cpu().numpy()

for i in range(test_edge_index.shape[1]):
    h, t = int(test_edge_index[0, i]), int(test_edge_index[1, i])
    r = int(test_edge_type[i])
    test_adjacency[h].append((t, r))
    test_reverse_adjacency[t].append((h, r))  # For bidirectional search if needed

print(f"  • Test adjacency: {len(test_adjacency)} nodes with outgoing edges")
print(f"  • Test reverse adjacency: {len(test_reverse_adjacency)} nodes with incoming edges")

# Instantiating test cases
path_finding_cases = instantiate_multihop_cases(BENCHMARK_QUESTIONS, test_adjacency, max_per_pattern=5)  # Increased for more robustness

print(f"   Instantiated {len(path_finding_cases)} test cases")

# Counting by type and difficulty for rigour
type_counts = Counter(c['type'] for c in path_finding_cases)
diff_counts = Counter(c['difficulty'] for c in path_finding_cases)
print(f"\n  Test cases by type: {dict(type_counts)}")
print(f"  Test cases by difficulty: {dict(diff_counts)}")

# ================================
# BASELINE IMPLEMENTATIONS
# ================================

print("\n Initialising methods for comparison...")

class TransEGreedyBaseline:
    """TransE-based greedy search baseline."""
    def __init__(self, entity_embs, relation_embs, adjacency):
        self.entity_embs = F.normalize(entity_embs.cpu(), p=2, dim=1)
        self.relation_embs = F.normalize(relation_embs.cpu(), p=2, dim=1)
        self.adjacency = adjacency
        self.name = "TransE-Greedy"

    def retrieve(self, source, target, question, max_hops=5):
        if source >= len(self.entity_embs) or target >= len(self.entity_embs):
            return {'found': False, 'path': None, 'length': 0, 'confidence': 0.0}

        path = [source]
        current = source
        visited = {source}

        for hop in range(max_hops):
            if current == target:
                return {'found': True, 'path': path, 'length': len(path)-1, 'confidence': 1.0 / (1 + hop)}

            best_next, best_score = None, float('inf')
            target_emb = self.entity_embs[target]

            for neighbor, rel in self.adjacency.get(current, []):
                if neighbor not in visited and neighbor < len(self.entity_embs):
                    score = torch.norm(self.entity_embs[current] + self.relation_embs[rel] - target_emb, p=2).item()
                    if score < best_score:
                        best_score = score
                        best_next = neighbor

            if best_next is None:
                break

            path.append(best_next)
            current = best_next
            visited.add(best_next)

        found = current == target
        return {'found': found, 'path': path if found else None, 'length': len(path)-1 if found else 0,
                'confidence': 1.0 / (1 + best_score) if found else 0.0}

class EmbeddingBeamSearch:
    """Embedding-based beam search baseline."""
    def __init__(self, entity_embs, adjacency, beam_width=15):
        self.entity_embs = F.normalize(entity_embs.cpu(), p=2, dim=1)
        self.adjacency = adjacency
        self.beam_width = beam_width
        self.name = "Embedding-Beam"

    def retrieve(self, source, target, question, max_hops=5):
        if source >= len(self.entity_embs) or target >= len(self.entity_embs):
            return {'found': False, 'path': None, 'length': 0, 'confidence': 0.0}

        target_emb = self.entity_embs[target]
        beam = [(0.0, [source])]  # (cumulative_score, path)
        visited = set()

        for hop in range(max_hops):
            new_beam = []

            for score, path in beam:
                current = path[-1]
                if current == target:
                    return {'found': True, 'path': path, 'length': len(path)-1, 'confidence': 1.0 / (1 + score)}

                path_key = tuple(path)
                if path_key in visited:
                    continue
                visited.add(path_key)

                for neighbor, _ in self.adjacency.get(current, []):
                    if neighbor not in path and neighbor < len(self.entity_embs):
                        sim = torch.dot(self.entity_embs[neighbor], target_emb).item()
                        new_score = score + (1 - sim)
                        new_beam.append((new_score, path + [neighbor]))

            beam = sorted(new_beam, key=lambda x: x[0])[:self.beam_width]
            if not beam:
                break

        # Checking if any in final beam reaches target
        for score, path in beam:
            if path[-1] == target:
                return {'found': True, 'path': path, 'length': len(path)-1, 'confidence': 1.0 / (1 + score)}

        return {'found': False, 'path': None, 'length': 0, 'confidence': 0.0}

class BFSBaseline:
    """BFS baseline for path finding."""
    def __init__(self, adjacency):
        self.adjacency = adjacency
        self.name = "BFS"

    def retrieve(self, source, target, question, max_hops=5):
        if source == target:
            return {'found': True, 'path': [source], 'length': 0, 'confidence': 1.0}

        visited = {source}
        queue = [([source], 0)]  # (path, dist)

        while queue:
            path, dist = queue.pop(0)
            current = path[-1]

            if dist >= max_hops:
                continue

            for neighbor, _ in self.adjacency.get(current, []):
                if neighbor == target:
                    final_path = path + [neighbor]
                    return {'found': True, 'path': final_path, 'length': len(final_path)-1,
                            'confidence': 1.0 / len(final_path)}

                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((path + [neighbor], dist + 1))

        return {'found': False, 'path': None, 'length': 0, 'confidence': 0.0}

class GNNRAGSystem:
    """GNN-RAG wrapper."""
    def __init__(self, retriever):
        self.retriever = retriever
        self.name = "GNN-RAG (Ours)"

    def retrieve(self, source, target, question, max_hops=5):
        paths = self.retriever.path_finder.find_paths_bidirectional(
            start=source, target=target, question_emb=None,
            max_hops=max_hops, beam_width=60)

        if paths:
            valid_paths = [(p, r, s) for p, r, s in paths if s > 0.1 and p[-1] == target]
            if valid_paths:
                best_path, _, score = max(valid_paths, key=lambda x: x[2])  # Selecting highest score
                return {'found': True, 'path': best_path, 'length': len(best_path)-1, 'confidence': score}

        return {'found': False, 'path': None, 'length': 0, 'confidence': 0.0}

# Initialising methods using TRAIN adjacency/embs (fair comparison, no test leakage)
train_adjacency = defaultdict(list)
train_edge_index = SPLIT_DATA['train_edge_index'].cpu().numpy()
train_edge_type = SPLIT_DATA['train_edge_type'].cpu().numpy()

for i in range(train_edge_index.shape[1]):
    h, t, r = int(train_edge_index[0, i]), int(train_edge_index[1, i]), int(train_edge_type[i])
    train_adjacency[h].append((t, r))

methods = {
    'GNN-RAG': GNNRAGSystem(RETRIEVER),
    'TransE-Greedy': TransEGreedyBaseline(TRAINED_ENTITY_EMBS, TRAINED_RELATION_EMBS, train_adjacency),
    'Embedding-Beam': EmbeddingBeamSearch(TRAINED_ENTITY_EMBS, train_adjacency, beam_width=15),
    'BFS': BFSBaseline(train_adjacency)}

print(f"   Initialised {len(methods)} methods")

# ================================
# RUN EVALUATION
# ================================

print("\n Running evaluation on {len(path_finding_cases)} test cases...")

# FORCING FRESH RETRIEVAL
print("⚠ Re-initialising GNN-RAG with updated scoring...")
methods['GNN-RAG'] = GNNRAGSystem(RETRIEVER)
print("✓ GNN-RAG reloaded with new path scoring weights")

results = defaultdict(lambda: {
    'found': [], 'path_length': [], 'time': [],
    'correct_distance': [], 'confidence': [],
    'by_difficulty': defaultdict(list),
    'by_type': defaultdict(list)})

for test in tqdm(path_finding_cases, desc="Evaluating"):
    source = test['source']
    target = test.get('target')  # Some types like similarity may not have target
    true_distance = test['true_distance']
    question = test['question']
    difficulty = test['difficulty']
    qtype = test['type']

    # Skipping or adapting for types without target (e.g., similarity/neighborhood)
    if target is None:
        # For simplicity, skipping non-path types in this eval; extending in future
        continue

    for method_name, method in methods.items():
        start_time = time.perf_counter()
        try:
            result = method.retrieve(source, target, question, max_hops=5)
            elapsed = time.perf_counter() - start_time

            found = 1 if result['found'] else 0
            results[method_name]['found'].append(found)
            results[method_name]['by_difficulty'][difficulty].append(found)
            results[method_name]['by_type'][qtype].append(found)
            results[method_name]['time'].append(elapsed)
            results[method_name]['confidence'].append(result['confidence'])

            if result['found']:
                path_len = result['length']
                results[method_name]['path_length'].append(path_len)
                if true_distance != float('inf'):
                    correct = 1 if path_len == true_distance else 0
                    results[method_name]['correct_distance'].append(correct)
            else:
                results[method_name]['path_length'].append(0)
                if true_distance != float('inf'):
                    results[method_name]['correct_distance'].append(0)

        except Exception as e:
            print(f"Warning: {method_name} failed on case: {e}")
            results[method_name]['found'].append(0)
            results[method_name]['by_difficulty'][difficulty].append(0)
            results[method_name]['by_type'][qtype].append(0)
            results[method_name]['path_length'].append(0)
            results[method_name]['time'].append(0)
            results[method_name]['confidence'].append(0)
            if true_distance != float('inf'):
                results[method_name]['correct_distance'].append(0)

# ================================
# COMPUTING METRICS
# ================================

print("\n" + "="*50)
print(" EVALUATION RESULTS")
print("="*50)

final_metrics = {}

for method_name in methods:
    success_rate = np.mean(results[method_name]['found']) * 100 if results[method_name]['found'] else 0

    successful_lengths = [l for l in results[method_name]['path_length'] if l > 0]
    avg_path_length = np.mean(successful_lengths) if successful_lengths else 0

    optimal_rate = np.mean(results[method_name]['correct_distance']) * 100 if results[method_name]['correct_distance'] else 0

    avg_time = np.mean(results[method_name]['time'])
    avg_confidence = np.mean(results[method_name]['confidence'])

    success_by_diff = {diff: np.mean(results[method_name]['by_difficulty'][diff]) * 100
                       for diff in results[method_name]['by_difficulty'] if results[method_name]['by_difficulty'][diff]}

    success_by_type = {typ: np.mean(results[method_name]['by_type'][typ]) * 100
                       for typ in results[method_name]['by_type'] if results[method_name]['by_type'][typ]}

    final_metrics[method_name] = {
        'success_rate': success_rate,
        'avg_path_length': avg_path_length,
        'optimal_rate': optimal_rate,
        'avg_time': avg_time,
        'avg_confidence': avg_confidence,
        'success_by_difficulty': success_by_diff,
        'success_by_type': success_by_type}

    print(f"\n{method_name}:")
    print(f"  Overall Success Rate: {success_rate:.1f}%")
    print(f"  Optimal Path Rate: {optimal_rate:.1f}%")
    print(f"  Avg Path Length: {avg_path_length:.2f}")
    print(f"  Avg Time: {avg_time:.4f}s")
    print(f"  Avg Confidence: {avg_confidence:.3f}")
    print(f"  Success by Type: { {k: f'{v:.1f}%' for k,v in success_by_type.items()} }")
    print(f"  Success by Difficulty: { {k: f'{v:.1f}%' for k,v in success_by_diff.items()} }")

# ================================
# STATISTICAL SIGNIFICANCE
# ================================

print("\n" + "="*50)
print(" STATISTICAL SIGNIFICANCE (vs GNN-RAG)")
print("="*50)

gnn_found = results['GNN-RAG']['found']

for method_name in methods:
    if method_name == 'GNN-RAG':
        continue

    other_found = results[method_name]['found']

    both_found = sum(1 for g, o in zip(gnn_found, other_found) if g == 1 and o == 1)
    gnn_only = sum(1 for g, o in zip(gnn_found, other_found) if g == 1 and o == 0)
    other_only = sum(1 for g, o in zip(gnn_found, other_found) if g == 0 and o == 1)
    neither = sum(1 for g, o in zip(gnn_found, other_found) if g == 0 and o == 0)

    contingency = [[both_found, gnn_only], [other_only, neither]]

    if gnn_only + other_only > 0:
        result = mcnemar(contingency, exact=False, correction=True)
        p_value = result.pvalue
    else:
        p_value = 1.0

    print(f"\n{method_name}:")
    print(f"  Success Rate Difference: {final_metrics['GNN-RAG']['success_rate'] - final_metrics[method_name]['success_rate']:+.1f}%")
    print(f"  McNemar p-value: {p_value:.6f} {'(significant)' if p_value < 0.05 else '(not significant)'}")

# ================================
# VISUALISATION
# ================================

print("\n Generating visualisations...")

fig = plt.figure(figsize=(18, 10))
gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)

methods_list = list(final_metrics.keys())
colors = ['#2E7D32', '#D32F2F', '#1976D2', '#F57C00']  # Green for GNN-RAG, etc.

# 1. Overall Success Rate
ax1 = fig.add_subplot(gs[0, 0])
success_rates = [final_metrics[m]['success_rate'] for m in methods_list]
bars = ax1.bar(range(len(methods_list)), success_rates, color=colors)
ax1.set_title('Overall Success Rate', fontsize=14, fontweight='bold')
ax1.set_ylabel('Success Rate (%)')
ax1.set_xticks(range(len(methods_list)))
ax1.set_xticklabels([m.replace('-', '\n') for m in methods_list])
ax1.set_ylim(0, 110)
for i, val in enumerate(success_rates):
    ax1.text(i, val + 2, f'{val:.1f}%', ha='center')

# 2. Success by Type
ax2 = fig.add_subplot(gs[0, 1])
qtypes = sorted(set(c['type'] for c in path_finding_cases))
x = np.arange(len(qtypes))
width = 0.2
for i, m in enumerate(methods_list):
    values = [final_metrics[m]['success_by_type'].get(qt, 0) for qt in qtypes]
    ax2.bar(x + i*width, values, width, label=m, color=colors[i])
ax2.set_title('Success by Question Type')
ax2.set_ylabel('Success Rate (%)')
ax2.set_xticks(x + width*(len(methods_list)-1)/2)
ax2.set_xticklabels(qtypes)
ax2.legend()
ax2.set_ylim(0, 110)

# 3. Optimal Path Rate
ax3 = fig.add_subplot(gs[0, 2])
optimal_rates = [final_metrics[m]['optimal_rate'] for m in methods_list]
bars = ax3.bar(range(len(methods_list)), optimal_rates, color=colors)
ax3.set_title('Optimal Path Rate')
ax3.set_ylabel('Optimal Rate (%)')
ax3.set_xticks(range(len(methods_list)))
ax3.set_xticklabels([m.replace('-', '\n') for m in methods_list])
ax3.set_ylim(0, 110)
for i, val in enumerate(optimal_rates):
    ax3.text(i, val + 2, f'{val:.1f}%', ha='center')

# 4. Average Time
ax4 = fig.add_subplot(gs[1, 0])
times = [final_metrics[m]['avg_time'] for m in methods_list]
bars = ax4.bar(range(len(methods_list)), times, color=colors)
ax4.set_title('Average Retrieval Time')
ax4.set_ylabel('Time (s)')
ax4.set_xticks(range(len(methods_list)))
ax4.set_xticklabels([m.replace('-', '\n') for m in methods_list])
for i, val in enumerate(times):
    ax4.text(i, val + max(times)*0.02, f'{val:.4f}s', ha='center')

# 5. Performance Table
ax5 = fig.add_subplot(gs[1, 1:])
ax5.axis('off')
table_data = [['Method', 'Success', 'Optimal', 'Time', 'F1']]
for m in methods_list:
    prec = final_metrics[m]['optimal_rate'] / 100 if final_metrics[m]['success_rate'] > 0 else 0
    rec = final_metrics[m]['success_rate'] / 100
    f1 = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0
    table_data.append([m, f"{final_metrics[m]['success_rate']:.1f}%",
                       f"{final_metrics[m]['optimal_rate']:.1f}%",
                       f"{final_metrics[m]['avg_time']:.4f}s", f"{f1:.3f}"])

table_data = sorted(table_data[1:], key=lambda x: float(x[-1]), reverse=True)
table_data.insert(0, ['Method', 'Success', 'Optimal', 'Time', 'F1'])

table = ax5.table(cellText=table_data, loc='center', colWidths=[0.2]*5)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1.2, 1.2)
for i in range(5):
    table[(0, i)].set_facecolor('#E0E0E0')

ax5.set_title('Performance Ranking (Sorted by F1)')

plt.suptitle('GNN-RAG Reasoning Benchmark Evaluation', fontsize=16, y=0.98)
plt.savefig('gnn_rag_reasoning_benchmark.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n Visualisation saved to 'gnn_rag_reasoning_benchmark.png'")

# ================================
# ANALYSING SUBOPTIMAL PATHS
# ================================

print("\n" + "="*60)
print(" ANALYSING SUBOPTIMAL PATHS")
print("="*60)

suboptimal_cases = []
for test in path_finding_cases:
    if test['target'] is None:
        continue
    gnn_result = methods['GNN-RAG'].retrieve(test['source'], test['target'], test['question'])
    bfs_result = methods['BFS'].retrieve(test['source'], test['target'], test['question'])

    if gnn_result['found'] and bfs_result['found'] and gnn_result['length'] > bfs_result['length']:
        suboptimal_cases.append({
            'source': test['source'],
            'target': test['target'],
            'gnn_length': gnn_result['length'],
            'bfs_length': bfs_result['length']})

print(f"  Found {len(suboptimal_cases)} suboptimal cases ({len(suboptimal_cases)/len(path_finding_cases)*100:.1f}% of total)")
if suboptimal_cases:
    length_diffs = [c['gnn_length'] - c['bfs_length'] for c in suboptimal_cases]
    print(f"  Avg excess hops: {np.mean(length_diffs):.2f}")
    print(f"  Most common excess: {Counter(length_diffs).most_common(1)[0][0]}")

# ================================
# FINAL SUMMARY
# ================================

print("\n" + "="*60)
print(" STEP 8 SUMMARY")
print("="*60)

sorted_methods = sorted(final_metrics.items(), key=lambda x: x[1]['success_rate'], reverse=True)
print("\nRanking by Success Rate:")
for i, (m, metrics) in enumerate(sorted_methods, 1):
    print(f"  {i}. {m}: {metrics['success_rate']:.1f}% {'(Ours)' if m == 'GNN-RAG' else ''}")

gnn_metrics = final_metrics['GNN-RAG']
print(f"\nKey Findings:")
print(f"  - GNN-RAG Success: {gnn_metrics['success_rate']:.1f}%")
print(f"  - Optimal Paths: {gnn_metrics['optimal_rate']:.1f}%")
print(f"  - Avg Time: {gnn_metrics['avg_time']:.4f}s")
if sorted_methods[0][0] == 'GNN-RAG':
    improvement = gnn_metrics['success_rate'] - sorted_methods[1][1]['success_rate']
    print(f"  - Outperforms best baseline by {improvement:.1f}%")

print("\nThis evaluation enhances rigour by using pattern-based questions on test data,")
print("with balanced types/difficulties and negative cases.")

print("\n" + "="*60)
print(" STEP 8 COMPLETE")
print("="*60)

# ================================
# STEP 8.5: ABLATION STUDIES
# ================================
print("\n" + "="*60)
print(" STEP 8.5: Ablation Studies")
print("="*60)
print("\nTesting which components of GNN-RAG contribute to performance...")

# Saving original path finder settings
original_use_model = path_finder.use_model_path_score
original_beam = path_finder.beam_width_default

# Ablation 1: Without neural path scoring (length only)
print("\n1. Testing without neural path scoring (length-only)...")
path_finder.use_model_path_score = False

ablation_results = {}
for method_name, method in [("GNN-RAG (no neural scorer)", GNNRAGSystem(retriever))]:
    results_temp = defaultdict(lambda: {'found': [], 'path_length': [], 'time': []})

    for test in tqdm(path_finding_cases[:500], desc=f"Ablation 1"):  # Sampling 500 for speed
        if test.get('target') is None:
            continue
        result = method.retrieve(test['source'], test['target'], test['question'])
        results_temp[method_name]['found'].append(1 if result['found'] else 0)
        if result['found']:
            results_temp[method_name]['path_length'].append(result['length'])

    ablation_results['no_neural_scorer'] = {
        'success': np.mean(results_temp[method_name]['found']) * 100,
        'avg_length': np.mean(results_temp[method_name]['path_length']) if results_temp[method_name]['path_length'] else 0}

# Restoring setting
path_finder.use_model_path_score = original_use_model

# Ablation 2: Narrow beam width (less exploration)
print("\n2. Testing with narrow beam (beam_width=10)...")
path_finder.beam_width_default = 10

results_temp = defaultdict(lambda: {'found': [], 'path_length': []})
for test in tqdm(path_finding_cases[:500], desc="Ablation 2"):
    if test.get('target') is None:
        continue
    result = GNNRAGSystem(retriever).retrieve(test['source'], test['target'], test['question'])
    results_temp['narrow_beam']['found'].append(1 if result['found'] else 0)
    if result['found']:
        results_temp['narrow_beam']['path_length'].append(result['length'])

ablation_results['narrow_beam'] = {
    'success': np.mean(results_temp['narrow_beam']['found']) * 100,
    'avg_length': np.mean(results_temp['narrow_beam']['path_length']) if results_temp['narrow_beam']['path_length'] else 0}

# Ablation 3: Wide beam width (more exploration)
print("\n3. Testing with wide beam (beam_width=150)...")
path_finder.beam_width_default = 150

results_temp = defaultdict(lambda: {'found': [], 'path_length': []})
for test in tqdm(path_finding_cases[:500], desc="Ablation 3"):
    if test.get('target') is None:
        continue
    result = GNNRAGSystem(retriever).retrieve(test['source'], test['target'], test['question'])
    results_temp['wide_beam']['found'].append(1 if result['found'] else 0)
    if result['found']:
        results_temp['wide_beam']['path_length'].append(result['length'])

ablation_results['wide_beam'] = {
    'success': np.mean(results_temp['wide_beam']['found']) * 100,
    'avg_length': np.mean(results_temp['wide_beam']['path_length']) if results_temp['wide_beam']['path_length'] else 0}

# Restoring original beam width
path_finder.beam_width_default = original_beam

# Printing ablation results
print("\n" + "="*60)
print("ABLATION RESULTS (on 500 test cases)")
print("="*60)
print(f"\nBaseline (GNN-RAG full):     Success: {final_metrics['GNN-RAG']['success_rate']:.1f}%")
print(f"Without neural scorer:       Success: {ablation_results['no_neural_scorer']['success']:.1f}%")
print(f"Narrow beam (width=10):      Success: {ablation_results['narrow_beam']['success']:.1f}%")
print(f"Wide beam (width=150):       Success: {ablation_results['wide_beam']['success']:.1f}%")

print("\nKey Finding:")
if ablation_results['no_neural_scorer']['success'] < final_metrics['GNN-RAG']['success_rate'] - 5:
    print("  Neural path scoring contributes significantly to performance")
else:
    print("  Neural path scoring has minimal impact; length penalty dominates")

print("\n" + "="*60)
print(" STEP 8.5 COMPLETE")
print("="*60)

# ================================
# STEP 8.6: SEMANTIC QUALITY ANALYSIS
# ================================
print("\n" + "="*60)
print(" STEP 8.6: Semantic Path Quality Analysis")
print("="*60)
print("\nAnalysing whether GNN-RAG's paths have higher semantic coherence...")

def compute_path_coherence(path, entity_embeddings):
    """Computes average cosine similarity between consecutive entities"""
    if len(path) < 2:
        return 0.0

    coherence_scores = []
    for i in range(len(path) - 1):
        e1 = entity_embeddings[path[i]]
        e2 = entity_embeddings[path[i+1]]
        coherence_scores.append(float(np.dot(e1, e2)))

    return np.mean(coherence_scores)

def compute_relation_consistency(relations):
    """Measures how consistent relations are in a path"""
    if not relations or len(relations) < 2:
        return 1.0

    # Higher score if fewer unique relations (more consistent)
    unique_rels = len(set(relations))
    return 1.0 / unique_rels

# Comparing paths found by GNN-RAG vs BFS on same queries
print("\nComparing path quality on cases where both methods succeed...")

gnn_coherence = []
bfs_coherence = []
gnn_consistency = []
bfs_consistency = []

comparison_cases = []
for test in path_finding_cases:
    if test.get('target') is None:
        continue

    gnn_result = methods['GNN-RAG'].retrieve(test['source'], test['target'], test['question'])
    bfs_result = methods['BFS'].retrieve(test['source'], test['target'], test['question'])

    if gnn_result['found'] and bfs_result['found']:
        comparison_cases.append({
            'gnn_path': gnn_result['path'],
            'bfs_path': bfs_result['path'],
            'gnn_length': gnn_result['length'],
            'bfs_length': bfs_result['length']})

    if len(comparison_cases) >= min(300, len(path_finding_cases)):
        break

print(f"Found {len(comparison_cases)} cases where both methods succeed")

for case in tqdm(comparison_cases, desc="Computing quality metrics"):
    # GNN-RAG path quality
    gnn_coh = compute_path_coherence(case['gnn_path'], path_finder.entity_embeddings)
    gnn_coherence.append(gnn_coh)

    # BFS path quality
    bfs_coh = compute_path_coherence(case['bfs_path'], path_finder.entity_embeddings)
    bfs_coherence.append(bfs_coh)

# Statistical comparison
print("\n" + "="*60)
print("SEMANTIC QUALITY RESULTS")
print("="*60)

print(f"\nPath Coherence (cosine similarity between consecutive entities):")
print(f"  GNN-RAG: {np.mean(gnn_coherence):.4f} ± {np.std(gnn_coherence):.4f}")
print(f"  BFS:     {np.mean(bfs_coherence):.4f} ± {np.std(bfs_coherence):.4f}")

if np.mean(gnn_coherence) > np.mean(bfs_coherence):
    gnn_avg = np.mean(gnn_coherence)
    bfs_avg = np.mean(bfs_coherence)

    # Absolute difference (more intuitive)
    abs_diff = gnn_avg - bfs_avg

    # Only compute % if BFS is positive
    if bfs_avg > 0:
        pct_improvement = (abs_diff / bfs_avg) * 100
        print(f"  → GNN-RAG paths are {pct_improvement:.1f}% more coherent")
    else:
        print(f"  → GNN-RAG coherence: {gnn_avg:.4f} vs BFS: {bfs_avg:.4f}")
        print(f"  → Absolute coherence advantage: {abs_diff:.4f}")

# Wilcoxon test for significance
from scipy.stats import wilcoxon
stat, p_val = wilcoxon(gnn_coherence, bfs_coherence, alternative='greater')
print(f"\nStatistical test (Wilcoxon): p={p_val:.4f}")
if p_val < 0.05:
    print("  ✓ GNN-RAG coherence is significantly higher")
else:
    print("  ✗ Difference is not statistically significant")

# Analysis by path length difference
print("\n" + "="*60)
print("TRADE-OFF ANALYSIS")
print("="*60)

longer_cases = [c for c in comparison_cases if c['gnn_length'] > c['bfs_length']]
print(f"\nIn {len(longer_cases)} cases where GNN-RAG found longer paths:")
print(f"  Avg extra hops: {np.mean([c['gnn_length'] - c['bfs_length'] for c in longer_cases]):.2f}")

if longer_cases:
    longer_gnn_coh = [compute_path_coherence(c['gnn_path'], path_finder.entity_embeddings) for c in longer_cases]
    longer_bfs_coh = [compute_path_coherence(c['bfs_path'], path_finder.entity_embeddings) for c in longer_cases]

    print(f"  GNN-RAG coherence: {np.mean(longer_gnn_coh):.4f}")
    print(f"  BFS coherence:     {np.mean(longer_bfs_coh):.4f}")

    if np.mean(longer_gnn_coh) > np.mean(longer_bfs_coh):
        gnn_coh = np.mean(longer_gnn_coh)
        bfs_coh = np.mean(longer_bfs_coh)
        abs_gain = gnn_coh - bfs_coh

        print(f"  → GNN-RAG coherence: {gnn_coh:.4f} vs BFS: {bfs_coh:.4f}")
        print(f"  → Absolute coherence gain: +{abs_gain:.4f} (for {abs_gain/gnn_coh*100:.1f}% of GNN score)")

print("\n" + "="*60)
print(" STEP 8.6 COMPLETE")
print("="*60)